{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # Gets the current notebook directory\n",
    "src_dir = os.path.join(cur_dir, '../src')  # Constructs the path to the 'src' directory\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('../data/SA_all.csv', index_col=0)\n",
    "all_df = all_df.dropna(subset=['Aspect', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aspect_set = set(['amenities and location',\n",
    "                  'ease of use', 'accessibility and availability', \n",
    "                  'customer service', 'ease of use', 'price and cost', \n",
    "                  'charging speed and efficiency', 'reliability and maintenance', \n",
    "                  'compatibility and connectivity', 'queue and waiting time', \n",
    "                  'reliability and maintenance', 'user-interface and mobile app', \n",
    "                  'payment Options', 'price and cost', 'amenities and location', \n",
    "                  'payment Options', 'customer service', 'safety', \n",
    "                  'accessibility and availability', 'user-interface and mobile app', \n",
    "                  'charging speed and efficiency', 'compatibility and connectivity', \n",
    "                  'safety', 'queue and waiting time'])\n",
    "all_df = all_df[all_df['Aspect'].isin(Aspect_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id_map = all_df.drop_duplicates(subset=['lat', 'lng']).reset_index(drop=True).reset_index().\\\n",
    "    rename(columns={'index': 'station_id'})[['station_id', 'lat', 'lng']]\n",
    "station_id_map['station_id'] = station_id_map['station_id'] + 1\n",
    "results = all_df.merge(station_id_map, on=['lat', 'lng'], how='left')\n",
    "results_positive = results[results['Polarity'] == 'Positive'].pivot_table(\n",
    "    index=['station_id', 'Review'], columns=['Aspect'], values=['Polarity'], aggfunc='count')\n",
    "results_negative = results[results['Polarity'] == 'Negative'].pivot_table(\n",
    "    index=['station_id',  'Review'], columns=['Aspect'], values=['Polarity'], aggfunc='count')\n",
    "results_positive.fillna(0, inplace=True)\n",
    "results_negative.fillna(0, inplace=True)\n",
    "results_positive.columns = results_positive.columns.droplevel(0)\n",
    "results_negative.columns = results_negative.columns.droplevel(0)\n",
    "\n",
    "results_negative.columns = [\n",
    "    col+\"_negative\" for col in results_negative.columns]\n",
    "results_positive.columns = [\n",
    "    col+\"_positive\" for col in results_positive.columns]\n",
    "results_positive.reset_index(inplace=True)\n",
    "results_negative.reset_index(inplace=True)\n",
    "merged = pd.merge(results_positive, results_negative, on=[\n",
    "                  'station_id', 'Review'], how='outer')\n",
    "merged.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all = pd.read_csv('../data/SA_all_processed.csv', index_col=0)\n",
    "# train_data = data_all.sample(frac=0.8, random_state=200)\n",
    "# test_data = data_all.drop(train_data.index)\n",
    "\n",
    "# train_data.to_csv('../data/train.csv')\n",
    "# test_data.to_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences = []\n",
    "\n",
    "for _, row in merged.iterrows():\n",
    "    sentences.append(tokenizer.tokenize(row['Review'].lower()))\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in sentences:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in sentences\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<6892 unique tokens: ['boise', 'charging', 'electric', 'good', 'in']...>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary\n",
    "\n",
    "# dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('../data/comments.dict')  # store the dictionary, for future reference\n",
    "\n",
    "dictionary = corpora.Dictionary.load('../data/comments.dict')\n",
    "print(dictionary)\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a not found in the pre-trained embeddings\n",
      "and not found in the pre-trained embeddings\n",
      "of not found in the pre-trained embeddings\n",
      "to not found in the pre-trained embeddings\n",
      "145kw not found in the pre-trained embeddings\n",
      "10 not found in the pre-trained embeddings\n",
      "chargepoint not found in the pre-trained embeddings\n",
      "2022 not found in the pre-trained embeddings\n",
      "chademo not found in the pre-trained embeddings\n",
      "evgo not found in the pre-trained embeddings\n",
      "01 not found in the pre-trained embeddings\n",
      "100 not found in the pre-trained embeddings\n",
      "120 not found in the pre-trained embeddings\n",
      "30 not found in the pre-trained embeddings\n",
      "140kw not found in the pre-trained embeddings\n",
      "16 not found in the pre-trained embeddings\n",
      "109kw not found in the pre-trained embeddings\n",
      "250 not found in the pre-trained embeddings\n",
      "25 not found in the pre-trained embeddings\n",
      "27kwh not found in the pre-trained embeddings\n",
      "36 not found in the pre-trained embeddings\n",
      "gainesville not found in the pre-trained embeddings\n",
      "inop not found in the pre-trained embeddings\n",
      "20 not found in the pre-trained embeddings\n",
      "70 not found in the pre-trained embeddings\n",
      "publix not found in the pre-trained embeddings\n",
      "4hour not found in the pre-trained embeddings\n",
      "24 not found in the pre-trained embeddings\n",
      "15 not found in the pre-trained embeddings\n",
      "35 not found in the pre-trained embeddings\n",
      "fordpass not found in the pre-trained embeddings\n",
      "sheetz not found in the pre-trained embeddings\n",
      "150 not found in the pre-trained embeddings\n",
      "350 not found in the pre-trained embeddings\n",
      "walmarts not found in the pre-trained embeddings\n",
      "10min not found in the pre-trained embeddings\n",
      "11am not found in the pre-trained embeddings\n",
      "160kw not found in the pre-trained embeddings\n",
      "350kw not found in the pre-trained embeddings\n",
      "67kw not found in the pre-trained embeddings\n",
      "2023 not found in the pre-trained embeddings\n",
      "11 not found in the pre-trained embeddings\n",
      "200kw not found in the pre-trained embeddings\n",
      "22 not found in the pre-trained embeddings\n",
      "23 not found in the pre-trained embeddings\n",
      "76kw not found in the pre-trained embeddings\n",
      "niro not found in the pre-trained embeddings\n",
      "12 not found in the pre-trained embeddings\n",
      "45 not found in the pre-trained embeddings\n",
      "emeryville not found in the pre-trained embeddings\n",
      "100kw not found in the pre-trained embeddings\n",
      "adaptor not found in the pre-trained embeddings\n",
      "38 not found in the pre-trained embeddings\n",
      "02 not found in the pre-trained embeddings\n",
      "06 not found in the pre-trained embeddings\n",
      "centre not found in the pre-trained embeddings\n",
      "10am not found in the pre-trained embeddings\n",
      "150kw not found in the pre-trained embeddings\n",
      "60kw not found in the pre-trained embeddings\n",
      "250kwh not found in the pre-trained embeddings\n",
      "48 not found in the pre-trained embeddings\n",
      "teslas not found in the pre-trained embeddings\n",
      "18 not found in the pre-trained embeddings\n",
      "880 not found in the pre-trained embeddings\n",
      "plugshare not found in the pre-trained embeddings\n",
      "87 not found in the pre-trained embeddings\n",
      "90 not found in the pre-trained embeddings\n",
      "panera not found in the pre-trained embeddings\n",
      "50 not found in the pre-trained embeddings\n",
      "crossgates not found in the pre-trained embeddings\n",
      "dieselgate not found in the pre-trained embeddings\n",
      "evs not found in the pre-trained embeddings\n",
      "14 not found in the pre-trained embeddings\n",
      "120kw not found in the pre-trained embeddings\n",
      "250kw not found in the pre-trained embeddings\n",
      "8kw not found in the pre-trained embeddings\n",
      "j1772 not found in the pre-trained embeddings\n",
      "50kw not found in the pre-trained embeddings\n",
      "nordstrom not found in the pre-trained embeddings\n",
      "ioniq not found in the pre-trained embeddings\n",
      "95 not found in the pre-trained embeddings\n",
      "300 not found in the pre-trained embeddings\n",
      "69 not found in the pre-trained embeddings\n",
      "meijer not found in the pre-trained embeddings\n",
      "applebee not found in the pre-trained embeddings\n",
      "biggby not found in the pre-trained embeddings\n",
      "252 not found in the pre-trained embeddings\n",
      "60 not found in the pre-trained embeddings\n",
      "tonopah not found in the pre-trained embeddings\n",
      "34 not found in the pre-trained embeddings\n",
      "40 not found in the pre-trained embeddings\n",
      "2018 not found in the pre-trained embeddings\n",
      "2020 not found in the pre-trained embeddings\n",
      "147kw not found in the pre-trained embeddings\n",
      "150w not found in the pre-trained embeddings\n",
      "50w not found in the pre-trained embeddings\n",
      "dcfc not found in the pre-trained embeddings\n",
      "530 not found in the pre-trained embeddings\n",
      "220kw not found in the pre-trained embeddings\n",
      "75 not found in the pre-trained embeddings\n",
      "45min not found in the pre-trained embeddings\n",
      "80 not found in the pre-trained embeddings\n",
      "200kwh not found in the pre-trained embeddings\n",
      "7kw not found in the pre-trained embeddings\n",
      "79 not found in the pre-trained embeddings\n",
      "40min not found in the pre-trained embeddings\n",
      "sunoco not found in the pre-trained embeddings\n",
      "110 not found in the pre-trained embeddings\n",
      "220 not found in the pre-trained embeddings\n",
      "41 not found in the pre-trained embeddings\n",
      "730am not found in the pre-trained embeddings\n",
      "petsmart not found in the pre-trained embeddings\n",
      "110kw not found in the pre-trained embeddings\n",
      "115 not found in the pre-trained embeddings\n",
      "72kw not found in the pre-trained embeddings\n",
      "marriott not found in the pre-trained embeddings\n",
      "250k not found in the pre-trained embeddings\n",
      "mashpee not found in the pre-trained embeddings\n",
      "sagamore not found in the pre-trained embeddings\n",
      "70kwh not found in the pre-trained embeddings\n",
      "115kw not found in the pre-trained embeddings\n",
      "150kwh not found in the pre-trained embeddings\n",
      "8kwh not found in the pre-trained embeddings\n",
      "98 not found in the pre-trained embeddings\n",
      "buc not found in the pre-trained embeddings\n",
      "21 not found in the pre-trained embeddings\n",
      "245kw not found in the pre-trained embeddings\n",
      "68 not found in the pre-trained embeddings\n",
      "1772 not found in the pre-trained embeddings\n",
      "40a not found in the pre-trained embeddings\n",
      "i10 not found in the pre-trained embeddings\n",
      "00 not found in the pre-trained embeddings\n",
      "13 not found in the pre-trained embeddings\n",
      "dothan not found in the pre-trained embeddings\n",
      "350s not found in the pre-trained embeddings\n",
      "bucees not found in the pre-trained embeddings\n",
      "f150 not found in the pre-trained embeddings\n",
      "29 not found in the pre-trained embeddings\n",
      "ev6 not found in the pre-trained embeddings\n",
      "500mi not found in the pre-trained embeddings\n",
      "75kw not found in the pre-trained embeddings\n",
      "id4 not found in the pre-trained embeddings\n",
      "395 not found in the pre-trained embeddings\n",
      "66 not found in the pre-trained embeddings\n",
      "65kwh not found in the pre-trained embeddings\n",
      "249 not found in the pre-trained embeddings\n",
      "37 not found in the pre-trained embeddings\n",
      "aldi not found in the pre-trained embeddings\n",
      "17 not found in the pre-trained embeddings\n",
      "30min not found in the pre-trained embeddings\n",
      "55 not found in the pre-trained embeddings\n",
      "30kw not found in the pre-trained embeddings\n",
      "170 not found in the pre-trained embeddings\n",
      "85 not found in the pre-trained embeddings\n",
      "abb not found in the pre-trained embeddings\n",
      "peasy not found in the pre-trained embeddings\n",
      "30am not found in the pre-trained embeddings\n",
      "abq not found in the pre-trained embeddings\n",
      "dyron not found in the pre-trained embeddings\n",
      "6kw not found in the pre-trained embeddings\n",
      "19 not found in the pre-trained embeddings\n",
      "85kw not found in the pre-trained embeddings\n",
      "ihop not found in the pre-trained embeddings\n",
      "80kw not found in the pre-trained embeddings\n",
      "46 not found in the pre-trained embeddings\n",
      "000 not found in the pre-trained embeddings\n",
      "110v not found in the pre-trained embeddings\n",
      "30k not found in the pre-trained embeddings\n",
      "84 not found in the pre-trained embeddings\n",
      "99 not found in the pre-trained embeddings\n",
      "999 not found in the pre-trained embeddings\n",
      "eightyfour not found in the pre-trained embeddings\n",
      "chadmo not found in the pre-trained embeddings\n",
      "800 not found in the pre-trained embeddings\n",
      "hyvee not found in the pre-trained embeddings\n",
      "arby not found in the pre-trained embeddings\n",
      "21kw not found in the pre-trained embeddings\n",
      "00pm not found in the pre-trained embeddings\n",
      "yellowstone not found in the pre-trained embeddings\n",
      "10x not found in the pre-trained embeddings\n",
      "ampup not found in the pre-trained embeddings\n",
      "protip not found in the pre-trained embeddings\n",
      "bosch not found in the pre-trained embeddings\n",
      "i70 not found in the pre-trained embeddings\n",
      "schnucks not found in the pre-trained embeddings\n",
      "125kw not found in the pre-trained embeddings\n",
      "03 not found in the pre-trained embeddings\n",
      "04 not found in the pre-trained embeddings\n",
      "13kw not found in the pre-trained embeddings\n",
      "galveston not found in the pre-trained embeddings\n",
      "59 not found in the pre-trained embeddings\n",
      "610 not found in the pre-trained embeddings\n",
      "2021 not found in the pre-trained embeddings\n",
      "100ft not found in the pre-trained embeddings\n",
      "48a not found in the pre-trained embeddings\n",
      "35kw not found in the pre-trained embeddings\n",
      "fpl not found in the pre-trained embeddings\n",
      "70kw not found in the pre-trained embeddings\n",
      "traveller not found in the pre-trained embeddings\n",
      "200 not found in the pre-trained embeddings\n",
      "i95 not found in the pre-trained embeddings\n",
      "20mins not found in the pre-trained embeddings\n",
      "chargingstation not found in the pre-trained embeddings\n",
      "150k not found in the pre-trained embeddings\n",
      "2011 not found in the pre-trained embeddings\n",
      "32 not found in the pre-trained embeddings\n",
      "34kw not found in the pre-trained embeddings\n",
      "78 not found in the pre-trained embeddings\n",
      "cosi not found in the pre-trained embeddings\n",
      "400mi not found in the pre-trained embeddings\n",
      "195 not found in the pre-trained embeddings\n",
      "495 not found in the pre-trained embeddings\n",
      "dsw not found in the pre-trained embeddings\n",
      "petco not found in the pre-trained embeddings\n",
      "qdoba not found in the pre-trained embeddings\n",
      "81 not found in the pre-trained embeddings\n",
      "binghamton not found in the pre-trained embeddings\n",
      "275 not found in the pre-trained embeddings\n",
      "doubletree not found in the pre-trained embeddings\n",
      "173 not found in the pre-trained embeddings\n",
      "khw not found in the pre-trained embeddings\n",
      "20kw not found in the pre-trained embeddings\n",
      "31 not found in the pre-trained embeddings\n",
      "50kwh not found in the pre-trained embeddings\n",
      "ioniq5 not found in the pre-trained embeddings\n",
      "elon not found in the pre-trained embeddings\n",
      "44kw not found in the pre-trained embeddings\n",
      "210kw not found in the pre-trained embeddings\n",
      "39 not found in the pre-trained embeddings\n",
      "r1t not found in the pre-trained embeddings\n",
      "rivian not found in the pre-trained embeddings\n",
      "1000 not found in the pre-trained embeddings\n",
      "isnt not found in the pre-trained embeddings\n",
      "whichwhich not found in the pre-trained embeddings\n",
      "1kwh not found in the pre-trained embeddings\n",
      "2kwh not found in the pre-trained embeddings\n",
      "140 not found in the pre-trained embeddings\n",
      "telsa not found in the pre-trained embeddings\n",
      "27 not found in the pre-trained embeddings\n",
      "30pm not found in the pre-trained embeddings\n",
      "71 not found in the pre-trained embeddings\n",
      "m3lr not found in the pre-trained embeddings\n",
      "evse not found in the pre-trained embeddings\n",
      "74 not found in the pre-trained embeddings\n",
      "bww not found in the pre-trained embeddings\n",
      "slc not found in the pre-trained embeddings\n",
      "dennys not found in the pre-trained embeddings\n",
      "v3s not found in the pre-trained embeddings\n",
      "bucee not found in the pre-trained embeddings\n",
      "230 not found in the pre-trained embeddings\n",
      "350kwh not found in the pre-trained embeddings\n",
      "beaumont not found in the pre-trained embeddings\n",
      "22nd not found in the pre-trained embeddings\n",
      "28 not found in the pre-trained embeddings\n",
      "1030 not found in the pre-trained embeddings\n",
      "24hr not found in the pre-trained embeddings\n",
      "76 not found in the pre-trained embeddings\n",
      "44 not found in the pre-trained embeddings\n",
      "49kw not found in the pre-trained embeddings\n",
      "breezewood not found in the pre-trained embeddings\n",
      "covid not found in the pre-trained embeddings\n",
      "64 not found in the pre-trained embeddings\n",
      "58 not found in the pre-trained embeddings\n",
      "92 not found in the pre-trained embeddings\n",
      "brownsville not found in the pre-trained embeddings\n",
      "starbase not found in the pre-trained embeddings\n",
      "palenque not found in the pre-trained embeddings\n",
      "10kw not found in the pre-trained embeddings\n",
      "39mph not found in the pre-trained embeddings\n",
      "120v not found in the pre-trained embeddings\n",
      "nema not found in the pre-trained embeddings\n",
      "150s not found in the pre-trained embeddings\n",
      "theatre not found in the pre-trained embeddings\n",
      "1100mi not found in the pre-trained embeddings\n",
      "10pm not found in the pre-trained embeddings\n",
      "125 not found in the pre-trained embeddings\n",
      "47 not found in the pre-trained embeddings\n",
      "gardnerville not found in the pre-trained embeddings\n",
      "myp not found in the pre-trained embeddings\n",
      "pacifica not found in the pre-trained embeddings\n",
      "truckee not found in the pre-trained embeddings\n",
      "chargepointdcfc not found in the pre-trained embeddings\n",
      "135kw not found in the pre-trained embeddings\n",
      "45am not found in the pre-trained embeddings\n",
      "240kw not found in the pre-trained embeddings\n",
      "250w not found in the pre-trained embeddings\n",
      "270 not found in the pre-trained embeddings\n",
      "abcd not found in the pre-trained embeddings\n",
      "86 not found in the pre-trained embeddings\n",
      "i80 not found in the pre-trained embeddings\n",
      "2019 not found in the pre-trained embeddings\n",
      "weis not found in the pre-trained embeddings\n",
      "24h not found in the pre-trained embeddings\n",
      "253kw not found in the pre-trained embeddings\n",
      "rochelle not found in the pre-trained embeddings\n",
      "365 not found in the pre-trained embeddings\n",
      "1500 not found in the pre-trained embeddings\n",
      "600 not found in the pre-trained embeddings\n",
      "zaxby not found in the pre-trained embeddings\n",
      "tanger not found in the pre-trained embeddings\n",
      "greenlots not found in the pre-trained embeddings\n",
      "obx not found in the pre-trained embeddings\n",
      "168 not found in the pre-trained embeddings\n",
      "chesapeake not found in the pre-trained embeddings\n",
      "15min not found in the pre-trained embeddings\n",
      "spotsylvania not found in the pre-trained embeddings\n",
      "47kw not found in the pre-trained embeddings\n",
      "43kw not found in the pre-trained embeddings\n",
      "burlington not found in the pre-trained embeddings\n",
      "205 not found in the pre-trained embeddings\n",
      "pge not found in the pre-trained embeddings\n",
      "2014 not found in the pre-trained embeddings\n",
      "tivoli not found in the pre-trained embeddings\n",
      "herbst not found in the pre-trained embeddings\n",
      "30mins not found in the pre-trained embeddings\n",
      "2016 not found in the pre-trained embeddings\n",
      "58kw not found in the pre-trained embeddings\n",
      "230kw not found in the pre-trained embeddings\n",
      "baymont not found in the pre-trained embeddings\n",
      "500 not found in the pre-trained embeddings\n",
      "05 not found in the pre-trained embeddings\n",
      "doughnuts not found in the pre-trained embeddings\n",
      "kreme not found in the pre-trained embeddings\n",
      "07 not found in the pre-trained embeddings\n",
      "72kwh not found in the pre-trained embeddings\n",
      "kwhr not found in the pre-trained embeddings\n",
      "smithfield not found in the pre-trained embeddings\n",
      "400 not found in the pre-trained embeddings\n",
      "57 not found in the pre-trained embeddings\n",
      "22kw not found in the pre-trained embeddings\n",
      "240 not found in the pre-trained embeddings\n",
      "wasnt not found in the pre-trained embeddings\n",
      "08 not found in the pre-trained embeddings\n",
      "75kwh not found in the pre-trained embeddings\n",
      "91 not found in the pre-trained embeddings\n",
      "170kw not found in the pre-trained embeddings\n",
      "didnt not found in the pre-trained embeddings\n",
      "gettysburg not found in the pre-trained embeddings\n",
      "kroger not found in the pre-trained embeddings\n",
      "taycan not found in the pre-trained embeddings\n",
      "49 not found in the pre-trained embeddings\n",
      "33 not found in the pre-trained embeddings\n",
      "7kwh not found in the pre-trained embeddings\n",
      "emmett not found in the pre-trained embeddings\n",
      "0kw not found in the pre-trained embeddings\n",
      "deming not found in the pre-trained embeddings\n",
      "26 not found in the pre-trained embeddings\n",
      "cabela not found in the pre-trained embeddings\n",
      "146 not found in the pre-trained embeddings\n",
      "43 not found in the pre-trained embeddings\n",
      "naperville not found in the pre-trained embeddings\n",
      "joliet not found in the pre-trained embeddings\n",
      "oswego not found in the pre-trained embeddings\n",
      "springhill not found in the pre-trained embeddings\n",
      "pdq not found in the pre-trained embeddings\n",
      "ccs1 not found in the pre-trained embeddings\n",
      "025 not found in the pre-trained embeddings\n",
      "30amp not found in the pre-trained embeddings\n",
      "42kw not found in the pre-trained embeddings\n",
      "197 not found in the pre-trained embeddings\n",
      "albertsons not found in the pre-trained embeddings\n",
      "240v not found in the pre-trained embeddings\n",
      "56 not found in the pre-trained embeddings\n",
      "56kw not found in the pre-trained embeddings\n",
      "90kw not found in the pre-trained embeddings\n",
      "94 not found in the pre-trained embeddings\n",
      "2012 not found in the pre-trained embeddings\n",
      "2013 not found in the pre-trained embeddings\n",
      "112 not found in the pre-trained embeddings\n",
      "42 not found in the pre-trained embeddings\n",
      "cinnabon not found in the pre-trained embeddings\n",
      "lubbock not found in the pre-trained embeddings\n",
      "72 not found in the pre-trained embeddings\n",
      "145 not found in the pre-trained embeddings\n",
      "16kw not found in the pre-trained embeddings\n",
      "euv not found in the pre-trained embeddings\n",
      "kws not found in the pre-trained embeddings\n",
      "goshen not found in the pre-trained embeddings\n",
      "sbarro not found in the pre-trained embeddings\n",
      "61 not found in the pre-trained embeddings\n",
      "32a not found in the pre-trained embeddings\n",
      "65 not found in the pre-trained embeddings\n",
      "woodbridge not found in the pre-trained embeddings\n",
      "100kwh not found in the pre-trained embeddings\n",
      "136kw not found in the pre-trained embeddings\n",
      "40kw not found in the pre-trained embeddings\n",
      "208v not found in the pre-trained embeddings\n",
      "hpwc not found in the pre-trained embeddings\n",
      "milford not found in the pre-trained embeddings\n",
      "88kw not found in the pre-trained embeddings\n",
      "3kw not found in the pre-trained embeddings\n",
      "chuy not found in the pre-trained embeddings\n",
      "bnb not found in the pre-trained embeddings\n",
      "64kw not found in the pre-trained embeddings\n",
      "50amp not found in the pre-trained embeddings\n",
      "autocharge not found in the pre-trained embeddings\n",
      "ll2 not found in the pre-trained embeddings\n",
      "62kw not found in the pre-trained embeddings\n",
      "iceing not found in the pre-trained embeddings\n",
      "eqs not found in the pre-trained embeddings\n",
      "wegmans not found in the pre-trained embeddings\n",
      "251kw not found in the pre-trained embeddings\n",
      "decatur not found in the pre-trained embeddings\n",
      "20th not found in the pre-trained embeddings\n",
      "112kw not found in the pre-trained embeddings\n",
      "75d not found in the pre-trained embeddings\n",
      "2kw not found in the pre-trained embeddings\n",
      "5kw not found in the pre-trained embeddings\n",
      "saginaw not found in the pre-trained embeddings\n",
      "raley not found in the pre-trained embeddings\n",
      "31kw not found in the pre-trained embeddings\n",
      "ariya not found in the pre-trained embeddings\n",
      "52kw not found in the pre-trained embeddings\n",
      "whataburger not found in the pre-trained embeddings\n",
      "6kwh not found in the pre-trained embeddings\n",
      "51 not found in the pre-trained embeddings\n",
      "96 not found in the pre-trained embeddings\n",
      "27th not found in the pre-trained embeddings\n",
      "350kv not found in the pre-trained embeddings\n",
      "harrah not found in the pre-trained embeddings\n",
      "chatge not found in the pre-trained embeddings\n",
      "electrifyamerica not found in the pre-trained embeddings\n",
      "i75 not found in the pre-trained embeddings\n",
      "tacobell not found in the pre-trained embeddings\n",
      "raton not found in the pre-trained embeddings\n",
      "55kw not found in the pre-trained embeddings\n",
      "200mi not found in the pre-trained embeddings\n",
      "phev not found in the pre-trained embeddings\n",
      "14kw not found in the pre-trained embeddings\n",
      "33kw not found in the pre-trained embeddings\n",
      "phx not found in the pre-trained embeddings\n",
      "202 not found in the pre-trained embeddings\n",
      "barstow not found in the pre-trained embeddings\n",
      "victorville not found in the pre-trained embeddings\n",
      "clarita not found in the pre-trained embeddings\n",
      "67 not found in the pre-trained embeddings\n",
      "69kw not found in the pre-trained embeddings\n",
      "murrieta not found in the pre-trained embeddings\n",
      "temecula not found in the pre-trained embeddings\n",
      "hannaford not found in the pre-trained embeddings\n",
      "160 not found in the pre-trained embeddings\n",
      "62 not found in the pre-trained embeddings\n",
      "phevs not found in the pre-trained embeddings\n",
      "visalia not found in the pre-trained embeddings\n",
      "30a not found in the pre-trained embeddings\n",
      "50k not found in the pre-trained embeddings\n",
      "homewood not found in the pre-trained embeddings\n",
      "212 not found in the pre-trained embeddings\n",
      "25mph not found in the pre-trained embeddings\n",
      "78kw not found in the pre-trained embeddings\n",
      "82 not found in the pre-trained embeddings\n",
      "mackenzie not found in the pre-trained embeddings\n",
      "28kw not found in the pre-trained embeddings\n",
      "custer not found in the pre-trained embeddings\n",
      "dakotamart not found in the pre-trained embeddings\n",
      "40mph not found in the pre-trained embeddings\n",
      "210v not found in the pre-trained embeddings\n",
      "rushmore not found in the pre-trained embeddings\n",
      "95kw not found in the pre-trained embeddings\n",
      "murdo not found in the pre-trained embeddings\n",
      "culvers not found in the pre-trained embeddings\n",
      "mcalister not found in the pre-trained embeddings\n",
      "84kw not found in the pre-trained embeddings\n",
      "chager not found in the pre-trained embeddings\n",
      "buccee not found in the pre-trained embeddings\n",
      "10th not found in the pre-trained embeddings\n",
      "walzem not found in the pre-trained embeddings\n",
      "valero not found in the pre-trained embeddings\n",
      "sonora not found in the pre-trained embeddings\n",
      "105 not found in the pre-trained embeddings\n",
      "12633 not found in the pre-trained embeddings\n",
      "420 not found in the pre-trained embeddings\n",
      "dorsett not found in the pre-trained embeddings\n",
      "fullso not found in the pre-trained embeddings\n",
      "mycharge not found in the pre-trained embeddings\n",
      "lvl2 not found in the pre-trained embeddings\n",
      "28th not found in the pre-trained embeddings\n",
      "newtonville not found in the pre-trained embeddings\n",
      "295 not found in the pre-trained embeddings\n",
      "sheels not found in the pre-trained embeddings\n",
      "scheels not found in the pre-trained embeddings\n",
      "schlotzky not found in the pre-trained embeddings\n",
      "wellsfargo not found in the pre-trained embeddings\n",
      "113kw not found in the pre-trained embeddings\n",
      "101 not found in the pre-trained embeddings\n",
      "chattanooga not found in the pre-trained embeddings\n",
      "gilmer not found in the pre-trained embeddings\n",
      "550 not found in the pre-trained embeddings\n",
      "woodmans not found in the pre-trained embeddings\n",
      "132 not found in the pre-trained embeddings\n",
      "25kw not found in the pre-trained embeddings\n",
      "brixx not found in the pre-trained embeddings\n",
      "mejier not found in the pre-trained embeddings\n",
      "greenwise not found in the pre-trained embeddings\n",
      "cees not found in the pre-trained embeddings\n",
      "130 not found in the pre-trained embeddings\n",
      "hiway not found in the pre-trained embeddings\n",
      "19kw not found in the pre-trained embeddings\n",
      "41kwh not found in the pre-trained embeddings\n",
      "54 not found in the pre-trained embeddings\n",
      "36kw not found in the pre-trained embeddings\n",
      "450 not found in the pre-trained embeddings\n",
      "brentwood not found in the pre-trained embeddings\n",
      "25am not found in the pre-trained embeddings\n",
      "63 not found in the pre-trained embeddings\n",
      "180 not found in the pre-trained embeddings\n",
      "popeyes not found in the pre-trained embeddings\n",
      "5stars not found in the pre-trained embeddings\n",
      "hortons not found in the pre-trained embeddings\n",
      "12kw not found in the pre-trained embeddings\n",
      "74kw not found in the pre-trained embeddings\n",
      "12a not found in the pre-trained embeddings\n",
      "90d not found in the pre-trained embeddings\n",
      "sedona not found in the pre-trained embeddings\n",
      "1abcd not found in the pre-trained embeddings\n",
      "2abcd not found in the pre-trained embeddings\n",
      "148kw not found in the pre-trained embeddings\n",
      "45kw not found in the pre-trained embeddings\n",
      "48kw not found in the pre-trained embeddings\n",
      "138 not found in the pre-trained embeddings\n",
      "folsom not found in the pre-trained embeddings\n",
      "120kwh not found in the pre-trained embeddings\n",
      "rhode not found in the pre-trained embeddings\n",
      "52 not found in the pre-trained embeddings\n",
      "easton not found in the pre-trained embeddings\n",
      "24hrs not found in the pre-trained embeddings\n",
      "semaconnect not found in the pre-trained embeddings\n",
      "kiwanis not found in the pre-trained embeddings\n",
      "huntsville not found in the pre-trained embeddings\n",
      "88 not found in the pre-trained embeddings\n",
      "fairfield not found in the pre-trained embeddings\n",
      "starlink not found in the pre-trained embeddings\n",
      "m3p not found in the pre-trained embeddings\n",
      "mcds not found in the pre-trained embeddings\n",
      "sinclair not found in the pre-trained embeddings\n",
      "i85 not found in the pre-trained embeddings\n",
      "155kw not found in the pre-trained embeddings\n",
      "82kw not found in the pre-trained embeddings\n",
      "71kw not found in the pre-trained embeddings\n",
      "datc not found in the pre-trained embeddings\n",
      "frisco not found in the pre-trained embeddings\n",
      "dcfcs not found in the pre-trained embeddings\n",
      "xc40 not found in the pre-trained embeddings\n",
      "caltrans not found in the pre-trained embeddings\n",
      "btc not found in the pre-trained embeddings\n",
      "chargepoints not found in the pre-trained embeddings\n",
      "63kw not found in the pre-trained embeddings\n",
      "egolf not found in the pre-trained embeddings\n",
      "50min not found in the pre-trained embeddings\n",
      "dadeland not found in the pre-trained embeddings\n",
      "107kw not found in the pre-trained embeddings\n",
      "winchester not found in the pre-trained embeddings\n",
      "tgi not found in the pre-trained embeddings\n",
      "200v not found in the pre-trained embeddings\n",
      "sonesta not found in the pre-trained embeddings\n",
      "evansville not found in the pre-trained embeddings\n",
      "37kw not found in the pre-trained embeddings\n",
      "yuma not found in the pre-trained embeddings\n",
      "120s not found in the pre-trained embeddings\n",
      "awc not found in the pre-trained embeddings\n",
      "190 not found in the pre-trained embeddings\n",
      "20min not found in the pre-trained embeddings\n",
      "waba not found in the pre-trained embeddings\n",
      "bojangles not found in the pre-trained embeddings\n",
      "alene not found in the pre-trained embeddings\n",
      "11pm not found in the pre-trained embeddings\n",
      "slowwww not found in the pre-trained embeddings\n",
      "125kwh not found in the pre-trained embeddings\n",
      "165 not found in the pre-trained embeddings\n",
      "travelling not found in the pre-trained embeddings\n",
      "etron not found in the pre-trained embeddings\n",
      "93 not found in the pre-trained embeddings\n",
      "teala not found in the pre-trained embeddings\n",
      "09 not found in the pre-trained embeddings\n",
      "92kw not found in the pre-trained embeddings\n",
      "pismo not found in the pre-trained embeddings\n",
      "17kw not found in the pre-trained embeddings\n",
      "chademos not found in the pre-trained embeddings\n",
      "encantada not found in the pre-trained embeddings\n",
      "kwph not found in the pre-trained embeddings\n",
      "applebees not found in the pre-trained embeddings\n",
      "airbnb not found in the pre-trained embeddings\n",
      "23kw not found in the pre-trained embeddings\n",
      "83 not found in the pre-trained embeddings\n",
      "instagram not found in the pre-trained embeddings\n",
      "rockport not found in the pre-trained embeddings\n",
      "evolveny not found in the pre-trained embeddings\n",
      "54kw not found in the pre-trained embeddings\n",
      "4kwh not found in the pre-trained embeddings\n",
      "4xe not found in the pre-trained embeddings\n",
      "suoercharger not found in the pre-trained embeddings\n",
      "centralia not found in the pre-trained embeddings\n",
      "kelso not found in the pre-trained embeddings\n",
      "tumwater not found in the pre-trained embeddings\n",
      "38kwh not found in the pre-trained embeddings\n",
      "nissans not found in the pre-trained embeddings\n",
      "firestone not found in the pre-trained embeddings\n",
      "48amps not found in the pre-trained embeddings\n",
      "378 not found in the pre-trained embeddings\n",
      "waterton not found in the pre-trained embeddings\n",
      "41kw not found in the pre-trained embeddings\n",
      "hardee not found in the pre-trained embeddings\n",
      "143 not found in the pre-trained embeddings\n",
      "103kw not found in the pre-trained embeddings\n",
      "11th not found in the pre-trained embeddings\n",
      "seneca not found in the pre-trained embeddings\n",
      "baskin not found in the pre-trained embeddings\n",
      "26kwh not found in the pre-trained embeddings\n",
      "clippercreek not found in the pre-trained embeddings\n",
      "evconnect not found in the pre-trained embeddings\n",
      "350w not found in the pre-trained embeddings\n",
      "corvallis not found in the pre-trained embeddings\n",
      "7charge not found in the pre-trained embeddings\n",
      "30c not found in the pre-trained embeddings\n",
      "pkwy not found in the pre-trained embeddings\n",
      "sebring not found in the pre-trained embeddings\n",
      "level2 not found in the pre-trained embeddings\n",
      "560 not found in the pre-trained embeddings\n",
      "575 not found in the pre-trained embeddings\n",
      "2015 not found in the pre-trained embeddings\n",
      "105kw not found in the pre-trained embeddings\n",
      "altamonte not found in the pre-trained embeddings\n",
      "66kw not found in the pre-trained embeddings\n",
      "53kw not found in the pre-trained embeddings\n",
      "jcpenny not found in the pre-trained embeddings\n",
      "sylvester not found in the pre-trained embeddings\n",
      "zyan not found in the pre-trained embeddings\n",
      "73 not found in the pre-trained embeddings\n",
      "310 not found in the pre-trained embeddings\n",
      "westin not found in the pre-trained embeddings\n",
      "citgo not found in the pre-trained embeddings\n",
      "acadia not found in the pre-trained embeddings\n",
      "12x not found in the pre-trained embeddings\n",
      "rdu not found in the pre-trained embeddings\n",
      "16th not found in the pre-trained embeddings\n",
      "meijers not found in the pre-trained embeddings\n",
      "12f not found in the pre-trained embeddings\n",
      "jinete not found in the pre-trained embeddings\n",
      "30amps not found in the pre-trained embeddings\n",
      "611 not found in the pre-trained embeddings\n",
      "tannersville not found in the pre-trained embeddings\n",
      "poconos not found in the pre-trained embeddings\n",
      "476 not found in the pre-trained embeddings\n",
      "i35 not found in the pre-trained embeddings\n",
      "arbys not found in the pre-trained embeddings\n",
      "00am not found in the pre-trained embeddings\n",
      "kingman not found in the pre-trained embeddings\n",
      "autozone not found in the pre-trained embeddings\n",
      "4kw not found in the pre-trained embeddings\n",
      "quartzsite not found in the pre-trained embeddings\n",
      "havasu not found in the pre-trained embeddings\n",
      "24kw not found in the pre-trained embeddings\n",
      "40pm not found in the pre-trained embeddings\n",
      "i90 not found in the pre-trained embeddings\n",
      "4ab not found in the pre-trained embeddings\n",
      "2017 not found in the pre-trained embeddings\n",
      "48kwh not found in the pre-trained embeddings\n",
      "45mins not found in the pre-trained embeddings\n",
      "50s not found in the pre-trained embeddings\n",
      "bakersfield not found in the pre-trained embeddings\n",
      "18mi not found in the pre-trained embeddings\n",
      "rvs not found in the pre-trained embeddings\n",
      "quickchek not found in the pre-trained embeddings\n",
      "9kw not found in the pre-trained embeddings\n",
      "251 not found in the pre-trained embeddings\n",
      "wendys not found in the pre-trained embeddings\n",
      "leesburg not found in the pre-trained embeddings\n",
      "bellingham not found in the pre-trained embeddings\n",
      "spokane not found in the pre-trained embeddings\n",
      "ellensburg not found in the pre-trained embeddings\n",
      "moab not found in the pre-trained embeddings\n",
      "farmington not found in the pre-trained embeddings\n",
      "700 not found in the pre-trained embeddings\n",
      "kinston not found in the pre-trained embeddings\n",
      "enmark not found in the pre-trained embeddings\n",
      "statesboro not found in the pre-trained embeddings\n",
      "25kwh not found in the pre-trained embeddings\n",
      "stonehaus not found in the pre-trained embeddings\n",
      "40miles not found in the pre-trained embeddings\n",
      "beaufort not found in the pre-trained embeddings\n",
      "33mi not found in the pre-trained embeddings\n",
      "137 not found in the pre-trained embeddings\n",
      "enid not found in the pre-trained embeddings\n",
      "akron not found in the pre-trained embeddings\n",
      "twards not found in the pre-trained embeddings\n",
      "30sec not found in the pre-trained embeddings\n",
      "695 not found in the pre-trained embeddings\n",
      "dollarama not found in the pre-trained embeddings\n",
      "qew not found in the pre-trained embeddings\n",
      "220v not found in the pre-trained embeddings\n",
      "macdonalds not found in the pre-trained embeddings\n",
      "cheyenne not found in the pre-trained embeddings\n",
      "48amp not found in the pre-trained embeddings\n",
      "500mph not found in the pre-trained embeddings\n",
      "177 not found in the pre-trained embeddings\n",
      "285 not found in the pre-trained embeddings\n",
      "42kwh not found in the pre-trained embeddings\n",
      "menards not found in the pre-trained embeddings\n",
      "440 not found in the pre-trained embeddings\n",
      "klamath not found in the pre-trained embeddings\n",
      "marianna not found in the pre-trained embeddings\n",
      "130kw not found in the pre-trained embeddings\n",
      "cademo not found in the pre-trained embeddings\n",
      "30m not found in the pre-trained embeddings\n",
      "menlo not found in the pre-trained embeddings\n",
      "madisonville not found in the pre-trained embeddings\n",
      "29kw not found in the pre-trained embeddings\n",
      "texaco not found in the pre-trained embeddings\n",
      "andersons not found in the pre-trained embeddings\n",
      "65kw not found in the pre-trained embeddings\n",
      "turlock not found in the pre-trained embeddings\n",
      "jorney not found in the pre-trained embeddings\n",
      "ceder not found in the pre-trained embeddings\n",
      "scipio not found in the pre-trained embeddings\n",
      "arhaus not found in the pre-trained embeddings\n",
      "ozark not found in the pre-trained embeddings\n",
      "towneplace not found in the pre-trained embeddings\n",
      "us1 not found in the pre-trained embeddings\n",
      "mirabito not found in the pre-trained embeddings\n",
      "287 not found in the pre-trained embeddings\n",
      "orangeburg not found in the pre-trained embeddings\n",
      "evses not found in the pre-trained embeddings\n",
      "125a not found in the pre-trained embeddings\n",
      "labelled not found in the pre-trained embeddings\n",
      "kearney not found in the pre-trained embeddings\n",
      "ikeda not found in the pre-trained embeddings\n",
      "dairyqueen not found in the pre-trained embeddings\n",
      "utica not found in the pre-trained embeddings\n",
      "61kw not found in the pre-trained embeddings\n",
      "kerrville not found in the pre-trained embeddings\n",
      "spinx not found in the pre-trained embeddings\n",
      "stuckey not found in the pre-trained embeddings\n",
      "77kw not found in the pre-trained embeddings\n",
      "j1722 not found in the pre-trained embeddings\n",
      "147 not found in the pre-trained embeddings\n",
      "11kw not found in the pre-trained embeddings\n",
      "ukiah not found in the pre-trained embeddings\n",
      "i84 not found in the pre-trained embeddings\n",
      "sheraton not found in the pre-trained embeddings\n",
      "15pm not found in the pre-trained embeddings\n",
      "116kw not found in the pre-trained embeddings\n",
      "32kw not found in the pre-trained embeddings\n",
      "easyforward not found in the pre-trained embeddings\n",
      "kyler not found in the pre-trained embeddings\n",
      "vallejo not found in the pre-trained embeddings\n",
      "hasnt not found in the pre-trained embeddings\n",
      "yosemite not found in the pre-trained embeddings\n",
      "108 not found in the pre-trained embeddings\n",
      "234v not found in the pre-trained embeddings\n",
      "38kw not found in the pre-trained embeddings\n",
      "hannafords not found in the pre-trained embeddings\n",
      "childress not found in the pre-trained embeddings\n",
      "lonoke not found in the pre-trained embeddings\n",
      "39kw not found in the pre-trained embeddings\n",
      "dempseys not found in the pre-trained embeddings\n",
      "chetola not found in the pre-trained embeddings\n",
      "40amp not found in the pre-trained embeddings\n",
      "36mph not found in the pre-trained embeddings\n",
      "owatonna not found in the pre-trained embeddings\n",
      "46kw not found in the pre-trained embeddings\n",
      "faribault not found in the pre-trained embeddings\n",
      "zefnet not found in the pre-trained embeddings\n",
      "model3 not found in the pre-trained embeddings\n",
      "sunridge not found in the pre-trained embeddings\n",
      "evergy not found in the pre-trained embeddings\n",
      "killian not found in the pre-trained embeddings\n",
      "middletown not found in the pre-trained embeddings\n",
      "97 not found in the pre-trained embeddings\n",
      "newhampshire not found in the pre-trained embeddings\n",
      "100d not found in the pre-trained embeddings\n",
      "vongs not found in the pre-trained embeddings\n",
      "580 not found in the pre-trained embeddings\n",
      "tanglewood not found in the pre-trained embeddings\n",
      "shilo not found in the pre-trained embeddings\n",
      "evcs not found in the pre-trained embeddings\n",
      "boscov not found in the pre-trained embeddings\n",
      "deptford not found in the pre-trained embeddings\n",
      "gv60 not found in the pre-trained embeddings\n",
      "durango not found in the pre-trained embeddings\n",
      "livonia not found in the pre-trained embeddings\n",
      "106 not found in the pre-trained embeddings\n",
      "brigham not found in the pre-trained embeddings\n",
      "ogden not found in the pre-trained embeddings\n",
      "cenex not found in the pre-trained embeddings\n",
      "lariche not found in the pre-trained embeddings\n",
      "lenoir not found in the pre-trained embeddings\n",
      "21st not found in the pre-trained embeddings\n",
      "bergstrom not found in the pre-trained embeddings\n",
      "giddings not found in the pre-trained embeddings\n",
      "hamptons not found in the pre-trained embeddings\n",
      "shaws not found in the pre-trained embeddings\n",
      "hayward not found in the pre-trained embeddings\n",
      "webasto not found in the pre-trained embeddings\n",
      "astoria not found in the pre-trained embeddings\n",
      "alamosa not found in the pre-trained embeddings\n",
      "glenwood not found in the pre-trained embeddings\n",
      "bradenton not found in the pre-trained embeddings\n",
      "98kw not found in the pre-trained embeddings\n",
      "pensacola not found in the pre-trained embeddings\n",
      "73kw not found in the pre-trained embeddings\n",
      "36kwh not found in the pre-trained embeddings\n",
      "pagosa not found in the pre-trained embeddings\n",
      "401 not found in the pre-trained embeddings\n",
      "horton not found in the pre-trained embeddings\n",
      "45pm not found in the pre-trained embeddings\n",
      "30s not found in the pre-trained embeddings\n",
      "vons not found in the pre-trained embeddings\n",
      "oakhurst not found in the pre-trained embeddings\n",
      "tenaya not found in the pre-trained embeddings\n",
      "50a not found in the pre-trained embeddings\n",
      "willowbrook not found in the pre-trained embeddings\n",
      "tillamook not found in the pre-trained embeddings\n",
      "winthrop not found in the pre-trained embeddings\n",
      "10kwh not found in the pre-trained embeddings\n",
      "creston not found in the pre-trained embeddings\n",
      "osoyoos not found in the pre-trained embeddings\n",
      "strasburg not found in the pre-trained embeddings\n",
      "dahlonega not found in the pre-trained embeddings\n",
      "24a not found in the pre-trained embeddings\n",
      "taos not found in the pre-trained embeddings\n",
      "24hours not found in the pre-trained embeddings\n",
      "flemington not found in the pre-trained embeddings\n",
      "timmies not found in the pre-trained embeddings\n",
      "lethbridge not found in the pre-trained embeddings\n",
      "1800 not found in the pre-trained embeddings\n",
      "johnsbury not found in the pre-trained embeddings\n",
      "susanville not found in the pre-trained embeddings\n",
      "sobeys not found in the pre-trained embeddings\n",
      "316 not found in the pre-trained embeddings\n",
      "30mi not found in the pre-trained embeddings\n",
      "autocamp not found in the pre-trained embeddings\n",
      "quance not found in the pre-trained embeddings\n",
      "100a not found in the pre-trained embeddings\n",
      "saskatchewan not found in the pre-trained embeddings\n",
      "wolseley not found in the pre-trained embeddings\n",
      "lidi not found in the pre-trained embeddings\n",
      "bryce not found in the pre-trained embeddings\n",
      "lyft not found in the pre-trained embeddings\n",
      "waycroft not found in the pre-trained embeddings\n",
      "100miles not found in the pre-trained embeddings\n",
      "35kwh not found in the pre-trained embeddings\n",
      "27kw not found in the pre-trained embeddings\n",
      "300mph not found in the pre-trained embeddings\n",
      "100mi not found in the pre-trained embeddings\n",
      "79kw not found in the pre-trained embeddings\n",
      "orland not found in the pre-trained embeddings\n",
      "57kw not found in the pre-trained embeddings\n",
      "reparked not found in the pre-trained embeddings\n",
      "jule not found in the pre-trained embeddings\n",
      "biloxi not found in the pre-trained embeddings\n",
      "156 not found in the pre-trained embeddings\n",
      "kwatts not found in the pre-trained embeddings\n",
      "doesnt not found in the pre-trained embeddings\n",
      "lawton not found in the pre-trained embeddings\n",
      "33rd not found in the pre-trained embeddings\n",
      "267 not found in the pre-trained embeddings\n",
      "petaluma not found in the pre-trained embeddings\n",
      "denys not found in the pre-trained embeddings\n",
      "800v not found in the pre-trained embeddings\n",
      "castleton not found in the pre-trained embeddings\n",
      "20kwh not found in the pre-trained embeddings\n",
      "414 not found in the pre-trained embeddings\n",
      "kenmore not found in the pre-trained embeddings\n",
      "226 not found in the pre-trained embeddings\n",
      "rmv not found in the pre-trained embeddings\n",
      "krcr not found in the pre-trained embeddings\n",
      "conroe not found in the pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model_path = '../data/GoogleNews-vectors-negative300.bin'  # Update this path\n",
    "pretrained_model = KeyedVectors.load_word2vec_format(model_path, binary=True)  # or binary=False for text format\n",
    "\n",
    "# Prepare the embedding matrix\n",
    "embedding_dim = pretrained_model.vector_size\n",
    "num_embeddings = len(dictionary)+1 # +1 for the unknown token\n",
    "embedding_matrix = torch.zeros(num_embeddings, embedding_dim)\n",
    "\n",
    "# Populate the embedding matrix\n",
    "for idx, word in dictionary.iteritems():\n",
    "    if word in pretrained_model:\n",
    "        embedding_matrix[idx] = torch.tensor(pretrained_model[word])\n",
    "    else:\n",
    "        # embedding_matrix[idx] = torch.zeros(embedding_dim)\n",
    "        print(f'{word} not found in the pre-trained embeddings')\n",
    "\n",
    "# Create the nn.Embedding layer and load your pre-trained embeddings\n",
    "embedding_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "embedding_layer.weight = nn.Parameter(embedding_matrix)\n",
    "embedding_layer.weight.requires_grad = False  # Optionally freeze the embeddings\n",
    "\n",
    "# Save the embedding layer\n",
    "torch.save(embedding_layer, '../data/embedding_layer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n",
    "        super(MyDataset, self).__init__()\n",
    "\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"')\n",
    "            next(reader, None)  # Skip the header\n",
    "            for idx, line in enumerate(reader):\n",
    "                # Assuming the first column is station_id, and the second column is the review text.\n",
    "                text = line[2].lower()\n",
    "                # Read the rest of the columns as labels (multi-label for each row)\n",
    "                label = [float(label) for label in line[3:]]  # Adjusted to read multiple labels\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = np.array(labels)  # Convert labels to a numpy array for easier handling\n",
    "\n",
    "        self.dict = corpora.Dictionary.load(dict_path)\n",
    "        # self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "        #                         usecols=[0]).values\n",
    "        # self.dict = list(self.dict.token2id.keys())\n",
    "        # self.dict = [word[0] for word in self.dict]\n",
    "        self.max_length_sentences = max_length_sentences\n",
    "        self.max_length_word = max_length_word\n",
    "        self.num_classes = self.labels.shape[1]  # Adjusted to get the number of classes from the label shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]  # Now label is a vector\n",
    "        text = self.texts[index]\n",
    "        document_encode = [\n",
    "            [self.dict.token2id[word] if word in self.dict.token2id else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            in sent_tokenize(text=text)]\n",
    "\n",
    "            # [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            # in sent_tokenize(text=text)]\n",
    "\n",
    "        for sentences in document_encode:\n",
    "            if len(sentences) < self.max_length_word:\n",
    "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
    "                sentences.extend(extended_words)\n",
    "\n",
    "        if len(document_encode) < self.max_length_sentences:\n",
    "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
    "                                  range(self.max_length_sentences - len(document_encode))]\n",
    "            document_encode.extend(extended_sentences)\n",
    "\n",
    "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
    "                          :self.max_length_sentences]\n",
    "\n",
    "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
    "        document_encode += 1\n",
    "\n",
    "        return document_encode.astype(np.int64), label.astype(np.float32)  # Ensure correct data type for labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttNet(nn.Module):\n",
    "    def __init__(self, word2vec_path, hidden_size=50):\n",
    "        super(WordAttNet, self).__init__()\n",
    "\n",
    "        # dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
    "        # dict_len, embed_size = dict.shape\n",
    "        # dict_len += 1\n",
    "        \n",
    "        # unknown_word = np.zeros((1, embed_size))\n",
    "        # dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n",
    "\n",
    "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
    "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "\n",
    "        self.lookup = torch.load(word2vec_path)\n",
    "        # self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
    "        \n",
    "        embed_size = self.lookup.embedding_dim\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "\n",
    "        self.word_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        output = self.lookup(input)\n",
    "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
    "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output,output.permute(1,0))\n",
    "\n",
    "        return output, h_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import matrix_mul, element_wise_mul\n",
    "\n",
    "class SentAttNet(nn.Module):\n",
    "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n",
    "        super(SentAttNet, self).__init__()\n",
    "\n",
    "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
    "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
    "\n",
    "        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
    "        # self.sent_softmax = nn.Softmax()\n",
    "        # self.fc_softmax = nn.Softmax()\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        self.sent_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        f_output, h_output = self.gru(input, hidden_state)\n",
    "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, h_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAttNet(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n",
    "                 max_sent_length, max_word_length):\n",
    "        super(HierAttNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.max_sent_length = max_sent_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n",
    "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
    "        self._init_hidden_state()\n",
    "\n",
    "    def _init_hidden_state(self, last_batch_size=None):\n",
    "        if last_batch_size:\n",
    "            batch_size = last_batch_size\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
    "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
    "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output_list = []\n",
    "        input = input.permute(1, 0, 2)\n",
    "        for i in input:\n",
    "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n",
    "            output_list.append(output)\n",
    "        output = torch.cat(output_list, 0)\n",
    "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import get_max_lengths, get_evaluation\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "else:\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "train_set = '../data/train.csv'\n",
    "test_set = '../data/test.csv'\n",
    "word2vec_path = '../data/embedding_layer.pth'\n",
    "dict_path = '../data/comments.dict'\n",
    "word_hidden_size = 50\n",
    "sent_hidden_size = 50\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "num_epoches = 2\n",
    "test_interval = 50\n",
    "es_min_delta = 0.0\n",
    "es_patience = 5\n",
    "\n",
    "training_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": True,\n",
    "                   \"drop_last\": True}\n",
    "\n",
    "test_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "\n",
    "\n",
    "max_word_length, max_sent_length = get_max_lengths(train_set)\n",
    "training_set = MyDataset(train_set, dict_path, max_sent_length, max_word_length)\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "test_set = MyDataset(test_set, dict_path, max_sent_length, max_word_length)\n",
    "test_generator = DataLoader(test_set, **test_params)\n",
    "model = HierAttNet(word_hidden_size, sent_hidden_size, batch_size, training_set.num_classes,\n",
    "                   word2vec_path, max_sent_length, max_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=0.01)\n",
    "best_loss = 1e5\n",
    "best_epoch = 0\n",
    "model.train()\n",
    "num_iter_per_epoch = len(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Iteration: 1/622, Lr: 0.01, Loss: 7.1073503494262695, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 2/622, Lr: 0.01, Loss: 7.470470428466797, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 3/622, Lr: 0.01, Loss: 6.692411422729492, Hamming Loss: 0.2734375\n",
      "Epoch: 1/2, Iteration: 4/622, Lr: 0.01, Loss: 6.638357639312744, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 5/622, Lr: 0.01, Loss: 5.674221992492676, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 6/622, Lr: 0.01, Loss: 5.744932651519775, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 7/622, Lr: 0.01, Loss: 5.647161960601807, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 8/622, Lr: 0.01, Loss: 9.793882369995117, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 9/622, Lr: 0.01, Loss: 5.908073902130127, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 10/622, Lr: 0.01, Loss: 7.512166976928711, Hamming Loss: 0.23697916666666666\n",
      "Epoch: 1/2, Iteration: 11/622, Lr: 0.01, Loss: 7.279176235198975, Hamming Loss: 0.2786458333333333\n",
      "Epoch: 1/2, Iteration: 12/622, Lr: 0.01, Loss: 8.009467124938965, Hamming Loss: 0.3411458333333333\n",
      "Epoch: 1/2, Iteration: 13/622, Lr: 0.01, Loss: 6.560047149658203, Hamming Loss: 0.2994791666666667\n",
      "Epoch: 1/2, Iteration: 14/622, Lr: 0.01, Loss: 8.008244514465332, Hamming Loss: 0.3229166666666667\n",
      "Epoch: 1/2, Iteration: 15/622, Lr: 0.01, Loss: 6.193287372589111, Hamming Loss: 0.2890625\n",
      "Epoch: 1/2, Iteration: 16/622, Lr: 0.01, Loss: 7.454349517822266, Hamming Loss: 0.28125\n",
      "Epoch: 1/2, Iteration: 17/622, Lr: 0.01, Loss: 7.881777763366699, Hamming Loss: 0.2526041666666667\n",
      "Epoch: 1/2, Iteration: 18/622, Lr: 0.01, Loss: 7.7364983558654785, Hamming Loss: 0.25\n",
      "Epoch: 1/2, Iteration: 19/622, Lr: 0.01, Loss: 6.674312114715576, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 20/622, Lr: 0.01, Loss: 7.1163434982299805, Hamming Loss: 0.2604166666666667\n",
      "Epoch: 1/2, Iteration: 21/622, Lr: 0.01, Loss: 6.5864691734313965, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 22/622, Lr: 0.01, Loss: 6.752619743347168, Hamming Loss: 0.2421875\n",
      "Epoch: 1/2, Iteration: 23/622, Lr: 0.01, Loss: 7.387235164642334, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 24/622, Lr: 0.01, Loss: 7.572854995727539, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 25/622, Lr: 0.01, Loss: 7.159111976623535, Hamming Loss: 0.2578125\n",
      "Epoch: 1/2, Iteration: 26/622, Lr: 0.01, Loss: 7.161332607269287, Hamming Loss: 0.23958333333333334\n",
      "Epoch: 1/2, Iteration: 27/622, Lr: 0.01, Loss: 6.81046724319458, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 28/622, Lr: 0.01, Loss: 8.942424774169922, Hamming Loss: 0.25\n",
      "Epoch: 1/2, Iteration: 29/622, Lr: 0.01, Loss: 6.704843044281006, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 30/622, Lr: 0.01, Loss: 6.752349376678467, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 31/622, Lr: 0.01, Loss: 7.7127604484558105, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 32/622, Lr: 0.01, Loss: 7.010927677154541, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 33/622, Lr: 0.01, Loss: 7.143948554992676, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 34/622, Lr: 0.01, Loss: 6.587221145629883, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 35/622, Lr: 0.01, Loss: 9.426194190979004, Hamming Loss: 0.2578125\n",
      "Epoch: 1/2, Iteration: 36/622, Lr: 0.01, Loss: 5.839804649353027, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 37/622, Lr: 0.01, Loss: 7.339818954467773, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 38/622, Lr: 0.01, Loss: 7.605373859405518, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 39/622, Lr: 0.01, Loss: 6.746535301208496, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 40/622, Lr: 0.01, Loss: 7.153807163238525, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 41/622, Lr: 0.01, Loss: 7.5913591384887695, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 42/622, Lr: 0.01, Loss: 7.181815147399902, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 43/622, Lr: 0.01, Loss: 7.308490753173828, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 44/622, Lr: 0.01, Loss: 7.060830116271973, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 45/622, Lr: 0.01, Loss: 6.62173318862915, Hamming Loss: 0.24479166666666666\n",
      "Epoch: 1/2, Iteration: 46/622, Lr: 0.01, Loss: 7.793262958526611, Hamming Loss: 0.24479166666666666\n",
      "Epoch: 1/2, Iteration: 47/622, Lr: 0.01, Loss: 6.551492691040039, Hamming Loss: 0.24479166666666666\n",
      "Epoch: 1/2, Iteration: 48/622, Lr: 0.01, Loss: 6.036248207092285, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 49/622, Lr: 0.01, Loss: 6.735902786254883, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 50/622, Lr: 0.01, Loss: 5.35847282409668, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 51/622, Lr: 0.01, Loss: 7.450775623321533, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 52/622, Lr: 0.01, Loss: 5.383014678955078, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 53/622, Lr: 0.01, Loss: 9.999706268310547, Hamming Loss: 0.23697916666666666\n",
      "Epoch: 1/2, Iteration: 54/622, Lr: 0.01, Loss: 8.461009979248047, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 55/622, Lr: 0.01, Loss: 8.363152503967285, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 56/622, Lr: 0.01, Loss: 6.583990573883057, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 57/622, Lr: 0.01, Loss: 7.7001519203186035, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 58/622, Lr: 0.01, Loss: 8.824163436889648, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 59/622, Lr: 0.01, Loss: 7.460373878479004, Hamming Loss: 0.2552083333333333\n",
      "Epoch: 1/2, Iteration: 60/622, Lr: 0.01, Loss: 6.981663227081299, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 61/622, Lr: 0.01, Loss: 9.439817428588867, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 62/622, Lr: 0.01, Loss: 7.188579559326172, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 63/622, Lr: 0.01, Loss: 6.471095085144043, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 64/622, Lr: 0.01, Loss: 7.403657913208008, Hamming Loss: 0.2604166666666667\n",
      "Epoch: 1/2, Iteration: 65/622, Lr: 0.01, Loss: 6.385152339935303, Hamming Loss: 0.2604166666666667\n",
      "Epoch: 1/2, Iteration: 66/622, Lr: 0.01, Loss: 6.605179786682129, Hamming Loss: 0.24479166666666666\n",
      "Epoch: 1/2, Iteration: 67/622, Lr: 0.01, Loss: 6.155529022216797, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 68/622, Lr: 0.01, Loss: 6.953141689300537, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 69/622, Lr: 0.01, Loss: 5.8056488037109375, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 70/622, Lr: 0.01, Loss: 7.525594711303711, Hamming Loss: 0.2578125\n",
      "Epoch: 1/2, Iteration: 71/622, Lr: 0.01, Loss: 5.965525150299072, Hamming Loss: 0.25\n",
      "Epoch: 1/2, Iteration: 72/622, Lr: 0.01, Loss: 6.53382682800293, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 73/622, Lr: 0.01, Loss: 6.110718727111816, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 74/622, Lr: 0.01, Loss: 7.686925411224365, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 75/622, Lr: 0.01, Loss: 8.61396598815918, Hamming Loss: 0.2552083333333333\n",
      "Epoch: 1/2, Iteration: 76/622, Lr: 0.01, Loss: 6.375583648681641, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 77/622, Lr: 0.01, Loss: 6.5684404373168945, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 78/622, Lr: 0.01, Loss: 7.875765800476074, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 79/622, Lr: 0.01, Loss: 6.450472831726074, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 80/622, Lr: 0.01, Loss: 6.324282646179199, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 81/622, Lr: 0.01, Loss: 6.3393378257751465, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 82/622, Lr: 0.01, Loss: 6.409520149230957, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 83/622, Lr: 0.01, Loss: 6.319265842437744, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 84/622, Lr: 0.01, Loss: 8.320491790771484, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 85/622, Lr: 0.01, Loss: 7.572724342346191, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 86/622, Lr: 0.01, Loss: 4.603203773498535, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 87/622, Lr: 0.01, Loss: 6.233597755432129, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 88/622, Lr: 0.01, Loss: 5.686385154724121, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 1/2, Iteration: 89/622, Lr: 0.01, Loss: 8.268122673034668, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 90/622, Lr: 0.01, Loss: 6.512887477874756, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 91/622, Lr: 0.01, Loss: 8.359015464782715, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 92/622, Lr: 0.01, Loss: 7.314279556274414, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 93/622, Lr: 0.01, Loss: 5.951503276824951, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 94/622, Lr: 0.01, Loss: 5.999091148376465, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 95/622, Lr: 0.01, Loss: 8.597787857055664, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 96/622, Lr: 0.01, Loss: 6.644305229187012, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 97/622, Lr: 0.01, Loss: 6.494843006134033, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 98/622, Lr: 0.01, Loss: 7.464421272277832, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 99/622, Lr: 0.01, Loss: 9.445974349975586, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 100/622, Lr: 0.01, Loss: 6.933553695678711, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 101/622, Lr: 0.01, Loss: 10.347675323486328, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 102/622, Lr: 0.01, Loss: 8.296085357666016, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 103/622, Lr: 0.01, Loss: 7.755619049072266, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 104/622, Lr: 0.01, Loss: 9.384212493896484, Hamming Loss: 0.2630208333333333\n",
      "Epoch: 1/2, Iteration: 105/622, Lr: 0.01, Loss: 4.680849552154541, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 106/622, Lr: 0.01, Loss: 6.719959735870361, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 107/622, Lr: 0.01, Loss: 7.0742387771606445, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 108/622, Lr: 0.01, Loss: 5.516658306121826, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 109/622, Lr: 0.01, Loss: 8.311715126037598, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 110/622, Lr: 0.01, Loss: 6.7995734214782715, Hamming Loss: 0.23958333333333334\n",
      "Epoch: 1/2, Iteration: 111/622, Lr: 0.01, Loss: 6.708024978637695, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 112/622, Lr: 0.01, Loss: 7.177667617797852, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 113/622, Lr: 0.01, Loss: 8.48452377319336, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 114/622, Lr: 0.01, Loss: 6.588153839111328, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 115/622, Lr: 0.01, Loss: 8.228907585144043, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 116/622, Lr: 0.01, Loss: 5.050314426422119, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 117/622, Lr: 0.01, Loss: 7.438921928405762, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 118/622, Lr: 0.01, Loss: 7.253602504730225, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 119/622, Lr: 0.01, Loss: 6.191466808319092, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 120/622, Lr: 0.01, Loss: 6.888490200042725, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 121/622, Lr: 0.01, Loss: 5.8257832527160645, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 122/622, Lr: 0.01, Loss: 6.222135066986084, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 123/622, Lr: 0.01, Loss: 7.175766944885254, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 124/622, Lr: 0.01, Loss: 6.423556327819824, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 125/622, Lr: 0.01, Loss: 6.097847938537598, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 126/622, Lr: 0.01, Loss: 7.048266410827637, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 127/622, Lr: 0.01, Loss: 6.457513332366943, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 128/622, Lr: 0.01, Loss: 6.614315986633301, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 129/622, Lr: 0.01, Loss: 7.26948356628418, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 130/622, Lr: 0.01, Loss: 6.467769622802734, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 1/2, Iteration: 131/622, Lr: 0.01, Loss: 6.754141807556152, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 132/622, Lr: 0.01, Loss: 6.867592811584473, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 133/622, Lr: 0.01, Loss: 6.581698894500732, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 134/622, Lr: 0.01, Loss: 6.8661322593688965, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 135/622, Lr: 0.01, Loss: 7.802191734313965, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 136/622, Lr: 0.01, Loss: 6.115605354309082, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 137/622, Lr: 0.01, Loss: 6.219187259674072, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 138/622, Lr: 0.01, Loss: 7.573653221130371, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 139/622, Lr: 0.01, Loss: 7.114870548248291, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 140/622, Lr: 0.01, Loss: 9.311331748962402, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 141/622, Lr: 0.01, Loss: 8.377951622009277, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 142/622, Lr: 0.01, Loss: 7.1094560623168945, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 143/622, Lr: 0.01, Loss: 8.597383499145508, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 144/622, Lr: 0.01, Loss: 6.505772113800049, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 145/622, Lr: 0.01, Loss: 5.492763519287109, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 146/622, Lr: 0.01, Loss: 7.958850383758545, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 147/622, Lr: 0.01, Loss: 5.747742652893066, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 148/622, Lr: 0.01, Loss: 6.08663272857666, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 149/622, Lr: 0.01, Loss: 6.81222677230835, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 150/622, Lr: 0.01, Loss: 6.981827259063721, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 151/622, Lr: 0.01, Loss: 6.121828079223633, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 152/622, Lr: 0.01, Loss: 5.88505220413208, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 153/622, Lr: 0.01, Loss: 6.913511753082275, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 154/622, Lr: 0.01, Loss: 5.712822437286377, Hamming Loss: 0.140625\n",
      "Epoch: 1/2, Iteration: 155/622, Lr: 0.01, Loss: 8.980278968811035, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 156/622, Lr: 0.01, Loss: 7.235417366027832, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 157/622, Lr: 0.01, Loss: 9.087539672851562, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 158/622, Lr: 0.01, Loss: 8.509197235107422, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 159/622, Lr: 0.01, Loss: 5.838439464569092, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 160/622, Lr: 0.01, Loss: 8.304338455200195, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 161/622, Lr: 0.01, Loss: 6.497239112854004, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 162/622, Lr: 0.01, Loss: 6.990449905395508, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 1/2, Iteration: 163/622, Lr: 0.01, Loss: 6.439268112182617, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 164/622, Lr: 0.01, Loss: 8.405107498168945, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 165/622, Lr: 0.01, Loss: 6.33717679977417, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 166/622, Lr: 0.01, Loss: 6.102516174316406, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 167/622, Lr: 0.01, Loss: 7.589990139007568, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 168/622, Lr: 0.01, Loss: 6.312309741973877, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 169/622, Lr: 0.01, Loss: 7.207446575164795, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 170/622, Lr: 0.01, Loss: 6.87934684753418, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 171/622, Lr: 0.01, Loss: 6.129205226898193, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 172/622, Lr: 0.01, Loss: 5.265805721282959, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 173/622, Lr: 0.01, Loss: 5.647443771362305, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 174/622, Lr: 0.01, Loss: 7.774468421936035, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 175/622, Lr: 0.01, Loss: 7.8657379150390625, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 176/622, Lr: 0.01, Loss: 5.6888909339904785, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 177/622, Lr: 0.01, Loss: 7.894848823547363, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 178/622, Lr: 0.01, Loss: 6.6006622314453125, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 179/622, Lr: 0.01, Loss: 9.590690612792969, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 180/622, Lr: 0.01, Loss: 7.4118733406066895, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 181/622, Lr: 0.01, Loss: 6.714512348175049, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 182/622, Lr: 0.01, Loss: 5.983295917510986, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 183/622, Lr: 0.01, Loss: 5.337653636932373, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 184/622, Lr: 0.01, Loss: 7.612613677978516, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 185/622, Lr: 0.01, Loss: 6.2240142822265625, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 186/622, Lr: 0.01, Loss: 8.114689826965332, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 187/622, Lr: 0.01, Loss: 6.3260908126831055, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 188/622, Lr: 0.01, Loss: 6.830533504486084, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 189/622, Lr: 0.01, Loss: 7.40264368057251, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 190/622, Lr: 0.01, Loss: 6.102233409881592, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 191/622, Lr: 0.01, Loss: 5.828660488128662, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 192/622, Lr: 0.01, Loss: 6.311581134796143, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 193/622, Lr: 0.01, Loss: 7.531287670135498, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 194/622, Lr: 0.01, Loss: 6.628011226654053, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 195/622, Lr: 0.01, Loss: 7.748055458068848, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 196/622, Lr: 0.01, Loss: 5.54525089263916, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 197/622, Lr: 0.01, Loss: 5.539003372192383, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 198/622, Lr: 0.01, Loss: 5.4000043869018555, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 199/622, Lr: 0.01, Loss: 6.437690734863281, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 200/622, Lr: 0.01, Loss: 7.093925476074219, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 201/622, Lr: 0.01, Loss: 6.611117362976074, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 202/622, Lr: 0.01, Loss: 7.310615062713623, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 203/622, Lr: 0.01, Loss: 5.246453285217285, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 204/622, Lr: 0.01, Loss: 7.085675239562988, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 205/622, Lr: 0.01, Loss: 6.612966060638428, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 206/622, Lr: 0.01, Loss: 6.104065418243408, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 207/622, Lr: 0.01, Loss: 5.640584468841553, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 208/622, Lr: 0.01, Loss: 9.237540245056152, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 209/622, Lr: 0.01, Loss: 6.305713653564453, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 210/622, Lr: 0.01, Loss: 7.128244400024414, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 211/622, Lr: 0.01, Loss: 6.329151153564453, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 212/622, Lr: 0.01, Loss: 7.214572906494141, Hamming Loss: 0.2421875\n",
      "Epoch: 1/2, Iteration: 213/622, Lr: 0.01, Loss: 7.004472732543945, Hamming Loss: 0.2421875\n",
      "Epoch: 1/2, Iteration: 214/622, Lr: 0.01, Loss: 6.718357563018799, Hamming Loss: 0.2421875\n",
      "Epoch: 1/2, Iteration: 215/622, Lr: 0.01, Loss: 7.794778823852539, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 216/622, Lr: 0.01, Loss: 5.3890275955200195, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 217/622, Lr: 0.01, Loss: 4.853680610656738, Hamming Loss: 0.25\n",
      "Epoch: 1/2, Iteration: 218/622, Lr: 0.01, Loss: 6.400909900665283, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 219/622, Lr: 0.01, Loss: 6.606795310974121, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 220/622, Lr: 0.01, Loss: 5.956665515899658, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 221/622, Lr: 0.01, Loss: 5.9428181648254395, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 222/622, Lr: 0.01, Loss: 6.639901638031006, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 223/622, Lr: 0.01, Loss: 6.724380016326904, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 224/622, Lr: 0.01, Loss: 7.888057231903076, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 225/622, Lr: 0.01, Loss: 7.0231242179870605, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 226/622, Lr: 0.01, Loss: 7.193748474121094, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 227/622, Lr: 0.01, Loss: 7.828212261199951, Hamming Loss: 0.24479166666666666\n",
      "Epoch: 1/2, Iteration: 228/622, Lr: 0.01, Loss: 7.028840065002441, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 229/622, Lr: 0.01, Loss: 5.192511558532715, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 230/622, Lr: 0.01, Loss: 6.3753180503845215, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 231/622, Lr: 0.01, Loss: 6.904658317565918, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 232/622, Lr: 0.01, Loss: 4.401707172393799, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 233/622, Lr: 0.01, Loss: 6.700197696685791, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 234/622, Lr: 0.01, Loss: 9.238250732421875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 235/622, Lr: 0.01, Loss: 6.637004852294922, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 236/622, Lr: 0.01, Loss: 8.870935440063477, Hamming Loss: 0.13802083333333334\n",
      "Epoch: 1/2, Iteration: 237/622, Lr: 0.01, Loss: 6.574954986572266, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 238/622, Lr: 0.01, Loss: 6.60578727722168, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 239/622, Lr: 0.01, Loss: 7.4258294105529785, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 240/622, Lr: 0.01, Loss: 7.885015487670898, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 241/622, Lr: 0.01, Loss: 7.022411823272705, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 242/622, Lr: 0.01, Loss: 5.727040767669678, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 243/622, Lr: 0.01, Loss: 5.010140895843506, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 244/622, Lr: 0.01, Loss: 6.175886154174805, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 245/622, Lr: 0.01, Loss: 7.569380283355713, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 246/622, Lr: 0.01, Loss: 5.538761615753174, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 247/622, Lr: 0.01, Loss: 7.825808048248291, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 248/622, Lr: 0.01, Loss: 6.956051826477051, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 249/622, Lr: 0.01, Loss: 7.44497013092041, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 250/622, Lr: 0.01, Loss: 6.06112813949585, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 251/622, Lr: 0.01, Loss: 9.090787887573242, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 252/622, Lr: 0.01, Loss: 7.9509453773498535, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 253/622, Lr: 0.01, Loss: 6.689291954040527, Hamming Loss: 0.2421875\n",
      "Epoch: 1/2, Iteration: 254/622, Lr: 0.01, Loss: 6.264786243438721, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 255/622, Lr: 0.01, Loss: 6.59835147857666, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 256/622, Lr: 0.01, Loss: 5.252842426300049, Hamming Loss: 0.2526041666666667\n",
      "Epoch: 1/2, Iteration: 257/622, Lr: 0.01, Loss: 5.365261077880859, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 1/2, Iteration: 258/622, Lr: 0.01, Loss: 6.714717388153076, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 259/622, Lr: 0.01, Loss: 6.030533790588379, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 260/622, Lr: 0.01, Loss: 6.2613525390625, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 261/622, Lr: 0.01, Loss: 8.570314407348633, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 262/622, Lr: 0.01, Loss: 5.11109733581543, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 263/622, Lr: 0.01, Loss: 5.20562744140625, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 264/622, Lr: 0.01, Loss: 6.5528106689453125, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 265/622, Lr: 0.01, Loss: 6.680401802062988, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 266/622, Lr: 0.01, Loss: 8.23364543914795, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 267/622, Lr: 0.01, Loss: 7.249139785766602, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 268/622, Lr: 0.01, Loss: 6.5355224609375, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 269/622, Lr: 0.01, Loss: 9.720511436462402, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 270/622, Lr: 0.01, Loss: 4.977075099945068, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 271/622, Lr: 0.01, Loss: 6.696875095367432, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 272/622, Lr: 0.01, Loss: 5.5396809577941895, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 273/622, Lr: 0.01, Loss: 9.223075866699219, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 274/622, Lr: 0.01, Loss: 7.012567520141602, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 1/2, Iteration: 275/622, Lr: 0.01, Loss: 7.478815078735352, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 276/622, Lr: 0.01, Loss: 5.412672519683838, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 277/622, Lr: 0.01, Loss: 7.550452709197998, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 278/622, Lr: 0.01, Loss: 5.453302383422852, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 279/622, Lr: 0.01, Loss: 6.966244220733643, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 280/622, Lr: 0.01, Loss: 7.260828971862793, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 281/622, Lr: 0.01, Loss: 7.632110595703125, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 282/622, Lr: 0.01, Loss: 5.62576150894165, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 283/622, Lr: 0.01, Loss: 6.069990634918213, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 284/622, Lr: 0.01, Loss: 7.604384899139404, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 285/622, Lr: 0.01, Loss: 7.557733058929443, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 286/622, Lr: 0.01, Loss: 5.461700439453125, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 287/622, Lr: 0.01, Loss: 7.761521339416504, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 288/622, Lr: 0.01, Loss: 8.645434379577637, Hamming Loss: 0.2552083333333333\n",
      "Epoch: 1/2, Iteration: 289/622, Lr: 0.01, Loss: 5.646803855895996, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 290/622, Lr: 0.01, Loss: 4.68175745010376, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 291/622, Lr: 0.01, Loss: 5.933743000030518, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 292/622, Lr: 0.01, Loss: 7.576835632324219, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 293/622, Lr: 0.01, Loss: 6.714104652404785, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 294/622, Lr: 0.01, Loss: 6.980922698974609, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 295/622, Lr: 0.01, Loss: 4.632221698760986, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 296/622, Lr: 0.01, Loss: 6.515355110168457, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 297/622, Lr: 0.01, Loss: 5.739818096160889, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 298/622, Lr: 0.01, Loss: 6.961289405822754, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 299/622, Lr: 0.01, Loss: 5.911872386932373, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 300/622, Lr: 0.01, Loss: 7.788127899169922, Hamming Loss: 0.23697916666666666\n",
      "Epoch: 1/2, Iteration: 301/622, Lr: 0.01, Loss: 7.471071720123291, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 302/622, Lr: 0.01, Loss: 5.922002792358398, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 303/622, Lr: 0.01, Loss: 6.400636196136475, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 304/622, Lr: 0.01, Loss: 7.6572747230529785, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 305/622, Lr: 0.01, Loss: 7.575508117675781, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 306/622, Lr: 0.01, Loss: 8.246193885803223, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 307/622, Lr: 0.01, Loss: 6.390161514282227, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 308/622, Lr: 0.01, Loss: 6.819736480712891, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 309/622, Lr: 0.01, Loss: 5.797706127166748, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 310/622, Lr: 0.01, Loss: 5.9258246421813965, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 311/622, Lr: 0.01, Loss: 7.330569744110107, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 312/622, Lr: 0.01, Loss: 5.443917274475098, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 313/622, Lr: 0.01, Loss: 5.85835075378418, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 314/622, Lr: 0.01, Loss: 4.923135280609131, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 1/2, Iteration: 315/622, Lr: 0.01, Loss: 6.769771099090576, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 316/622, Lr: 0.01, Loss: 5.4022393226623535, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 317/622, Lr: 0.01, Loss: 6.305388450622559, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 318/622, Lr: 0.01, Loss: 7.11991024017334, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 319/622, Lr: 0.01, Loss: 6.641960144042969, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 320/622, Lr: 0.01, Loss: 6.131100177764893, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 321/622, Lr: 0.01, Loss: 9.047367095947266, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 322/622, Lr: 0.01, Loss: 6.501920223236084, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 323/622, Lr: 0.01, Loss: 7.893863677978516, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 324/622, Lr: 0.01, Loss: 5.296682834625244, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 325/622, Lr: 0.01, Loss: 6.854922294616699, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 326/622, Lr: 0.01, Loss: 6.910345077514648, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 327/622, Lr: 0.01, Loss: 5.2839765548706055, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 328/622, Lr: 0.01, Loss: 7.676459789276123, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 329/622, Lr: 0.01, Loss: 5.9927873611450195, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 330/622, Lr: 0.01, Loss: 8.75894546508789, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 331/622, Lr: 0.01, Loss: 4.67230749130249, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 332/622, Lr: 0.01, Loss: 7.038924694061279, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 333/622, Lr: 0.01, Loss: 7.422543048858643, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 334/622, Lr: 0.01, Loss: 6.6547017097473145, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 335/622, Lr: 0.01, Loss: 7.554553508758545, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 336/622, Lr: 0.01, Loss: 8.030649185180664, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 337/622, Lr: 0.01, Loss: 5.205033302307129, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 338/622, Lr: 0.01, Loss: 7.343480587005615, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 339/622, Lr: 0.01, Loss: 6.600130081176758, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 340/622, Lr: 0.01, Loss: 6.15247106552124, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 341/622, Lr: 0.01, Loss: 7.557186603546143, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 342/622, Lr: 0.01, Loss: 5.894767761230469, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 343/622, Lr: 0.01, Loss: 6.192255020141602, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 344/622, Lr: 0.01, Loss: 7.105949401855469, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 345/622, Lr: 0.01, Loss: 7.567230701446533, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 346/622, Lr: 0.01, Loss: 5.352972030639648, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 347/622, Lr: 0.01, Loss: 8.443153381347656, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 348/622, Lr: 0.01, Loss: 5.986656188964844, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 349/622, Lr: 0.01, Loss: 5.246890544891357, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 350/622, Lr: 0.01, Loss: 7.979245185852051, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 351/622, Lr: 0.01, Loss: 7.702460289001465, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 352/622, Lr: 0.01, Loss: 6.308355808258057, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 353/622, Lr: 0.01, Loss: 6.895857810974121, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 354/622, Lr: 0.01, Loss: 7.126662254333496, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 355/622, Lr: 0.01, Loss: 6.488277435302734, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 356/622, Lr: 0.01, Loss: 6.001838684082031, Hamming Loss: 0.140625\n",
      "Epoch: 1/2, Iteration: 357/622, Lr: 0.01, Loss: 7.206632137298584, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 358/622, Lr: 0.01, Loss: 5.986810207366943, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 359/622, Lr: 0.01, Loss: 5.03115701675415, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 1/2, Iteration: 360/622, Lr: 0.01, Loss: 7.050974369049072, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 361/622, Lr: 0.01, Loss: 5.718027591705322, Hamming Loss: 0.1328125\n",
      "Epoch: 1/2, Iteration: 362/622, Lr: 0.01, Loss: 6.147263526916504, Hamming Loss: 0.1484375\n",
      "Epoch: 1/2, Iteration: 363/622, Lr: 0.01, Loss: 6.978878021240234, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 364/622, Lr: 0.01, Loss: 7.490776062011719, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 365/622, Lr: 0.01, Loss: 7.635441303253174, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 366/622, Lr: 0.01, Loss: 8.167510032653809, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 1/2, Iteration: 367/622, Lr: 0.01, Loss: 6.032816410064697, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 368/622, Lr: 0.01, Loss: 7.323254108428955, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 369/622, Lr: 0.01, Loss: 6.897489547729492, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 370/622, Lr: 0.01, Loss: 5.055288791656494, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 371/622, Lr: 0.01, Loss: 6.996834754943848, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 1/2, Iteration: 372/622, Lr: 0.01, Loss: 5.792693614959717, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 373/622, Lr: 0.01, Loss: 5.979088306427002, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 374/622, Lr: 0.01, Loss: 9.340691566467285, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 375/622, Lr: 0.01, Loss: 6.174859046936035, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 376/622, Lr: 0.01, Loss: 5.713070392608643, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 377/622, Lr: 0.01, Loss: 7.457618713378906, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 378/622, Lr: 0.01, Loss: 5.430150032043457, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 379/622, Lr: 0.01, Loss: 4.5080180168151855, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 380/622, Lr: 0.01, Loss: 6.243062496185303, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 381/622, Lr: 0.01, Loss: 6.063060760498047, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 382/622, Lr: 0.01, Loss: 6.646861553192139, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 383/622, Lr: 0.01, Loss: 8.213956832885742, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 384/622, Lr: 0.01, Loss: 6.548534870147705, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 385/622, Lr: 0.01, Loss: 3.810671806335449, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 386/622, Lr: 0.01, Loss: 8.120052337646484, Hamming Loss: 0.23697916666666666\n",
      "Epoch: 1/2, Iteration: 387/622, Lr: 0.01, Loss: 7.294103622436523, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 388/622, Lr: 0.01, Loss: 5.5951457023620605, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 389/622, Lr: 0.01, Loss: 8.940590858459473, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 390/622, Lr: 0.01, Loss: 6.0228590965271, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 391/622, Lr: 0.01, Loss: 6.135550022125244, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 392/622, Lr: 0.01, Loss: 6.543020248413086, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 393/622, Lr: 0.01, Loss: 5.304663181304932, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 394/622, Lr: 0.01, Loss: 5.429275035858154, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 395/622, Lr: 0.01, Loss: 7.129240989685059, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 396/622, Lr: 0.01, Loss: 7.579318523406982, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 397/622, Lr: 0.01, Loss: 4.568347454071045, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 398/622, Lr: 0.01, Loss: 3.8537423610687256, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 399/622, Lr: 0.01, Loss: 6.802428245544434, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 400/622, Lr: 0.01, Loss: 7.598051071166992, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 401/622, Lr: 0.01, Loss: 6.907716274261475, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 402/622, Lr: 0.01, Loss: 5.49572229385376, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 403/622, Lr: 0.01, Loss: 6.974569320678711, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 404/622, Lr: 0.01, Loss: 5.865539073944092, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 405/622, Lr: 0.01, Loss: 7.1660237312316895, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 406/622, Lr: 0.01, Loss: 6.010732173919678, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 407/622, Lr: 0.01, Loss: 6.469919204711914, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 408/622, Lr: 0.01, Loss: 6.384234428405762, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 409/622, Lr: 0.01, Loss: 5.7966461181640625, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 410/622, Lr: 0.01, Loss: 8.210206985473633, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 411/622, Lr: 0.01, Loss: 8.216346740722656, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 412/622, Lr: 0.01, Loss: 8.002922058105469, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 413/622, Lr: 0.01, Loss: 5.250997543334961, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 414/622, Lr: 0.01, Loss: 5.624467849731445, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 415/622, Lr: 0.01, Loss: 5.519417762756348, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 416/622, Lr: 0.01, Loss: 7.114290237426758, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 417/622, Lr: 0.01, Loss: 7.7610249519348145, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 418/622, Lr: 0.01, Loss: 6.685703277587891, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 419/622, Lr: 0.01, Loss: 6.657933235168457, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 420/622, Lr: 0.01, Loss: 7.910114765167236, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 421/622, Lr: 0.01, Loss: 6.19096565246582, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 422/622, Lr: 0.01, Loss: 3.594520092010498, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 1/2, Iteration: 423/622, Lr: 0.01, Loss: 6.578215599060059, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 424/622, Lr: 0.01, Loss: 5.81716251373291, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 425/622, Lr: 0.01, Loss: 6.228808403015137, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 426/622, Lr: 0.01, Loss: 4.292727947235107, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 427/622, Lr: 0.01, Loss: 6.002712726593018, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 428/622, Lr: 0.01, Loss: 8.102826118469238, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 429/622, Lr: 0.01, Loss: 6.733109474182129, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 430/622, Lr: 0.01, Loss: 5.498644828796387, Hamming Loss: 0.13541666666666666\n",
      "Epoch: 1/2, Iteration: 431/622, Lr: 0.01, Loss: 5.1254167556762695, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 432/622, Lr: 0.01, Loss: 8.13259220123291, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 433/622, Lr: 0.01, Loss: 5.81670618057251, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 434/622, Lr: 0.01, Loss: 7.132729530334473, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 435/622, Lr: 0.01, Loss: 7.710119247436523, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 436/622, Lr: 0.01, Loss: 6.879279136657715, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 437/622, Lr: 0.01, Loss: 7.41981315612793, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 438/622, Lr: 0.01, Loss: 6.559988975524902, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 439/622, Lr: 0.01, Loss: 8.590353012084961, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 440/622, Lr: 0.01, Loss: 7.604341506958008, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 441/622, Lr: 0.01, Loss: 7.994665145874023, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 442/622, Lr: 0.01, Loss: 4.987478256225586, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 443/622, Lr: 0.01, Loss: 6.277203559875488, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 444/622, Lr: 0.01, Loss: 5.182279586791992, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 445/622, Lr: 0.01, Loss: 4.961967945098877, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 446/622, Lr: 0.01, Loss: 4.916624546051025, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 447/622, Lr: 0.01, Loss: 4.750835418701172, Hamming Loss: 0.14583333333333334\n",
      "Epoch: 1/2, Iteration: 448/622, Lr: 0.01, Loss: 7.350770950317383, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 449/622, Lr: 0.01, Loss: 5.398130416870117, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 450/622, Lr: 0.01, Loss: 6.862432479858398, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 451/622, Lr: 0.01, Loss: 5.121728897094727, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 452/622, Lr: 0.01, Loss: 5.538122177124023, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 453/622, Lr: 0.01, Loss: 4.9292192459106445, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 454/622, Lr: 0.01, Loss: 5.318706512451172, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 455/622, Lr: 0.01, Loss: 5.841641426086426, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 456/622, Lr: 0.01, Loss: 3.7779736518859863, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 457/622, Lr: 0.01, Loss: 7.331994533538818, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 458/622, Lr: 0.01, Loss: 5.013503074645996, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 459/622, Lr: 0.01, Loss: 8.296904563903809, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 460/622, Lr: 0.01, Loss: 5.7635884284973145, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 461/622, Lr: 0.01, Loss: 5.978557109832764, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 462/622, Lr: 0.01, Loss: 6.889698028564453, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 463/622, Lr: 0.01, Loss: 6.084071159362793, Hamming Loss: 0.234375\n",
      "Epoch: 1/2, Iteration: 464/622, Lr: 0.01, Loss: 4.980420112609863, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 465/622, Lr: 0.01, Loss: 6.56490421295166, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 466/622, Lr: 0.01, Loss: 8.281733512878418, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 467/622, Lr: 0.01, Loss: 8.450263023376465, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 468/622, Lr: 0.01, Loss: 8.059901237487793, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 469/622, Lr: 0.01, Loss: 6.3055739402771, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 470/622, Lr: 0.01, Loss: 7.043123722076416, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 471/622, Lr: 0.01, Loss: 6.955220699310303, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 472/622, Lr: 0.01, Loss: 5.20732307434082, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 473/622, Lr: 0.01, Loss: 7.422966480255127, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 474/622, Lr: 0.01, Loss: 4.78105354309082, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 475/622, Lr: 0.01, Loss: 5.381791591644287, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 476/622, Lr: 0.01, Loss: 6.640097141265869, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 477/622, Lr: 0.01, Loss: 4.661284446716309, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 478/622, Lr: 0.01, Loss: 5.1629180908203125, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 479/622, Lr: 0.01, Loss: 7.079610347747803, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 480/622, Lr: 0.01, Loss: 5.579804420471191, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 481/622, Lr: 0.01, Loss: 6.673187255859375, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 482/622, Lr: 0.01, Loss: 5.727499961853027, Hamming Loss: 0.2630208333333333\n",
      "Epoch: 1/2, Iteration: 483/622, Lr: 0.01, Loss: 6.934641361236572, Hamming Loss: 0.24739583333333334\n",
      "Epoch: 1/2, Iteration: 484/622, Lr: 0.01, Loss: 5.3747758865356445, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 485/622, Lr: 0.01, Loss: 5.99138069152832, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 486/622, Lr: 0.01, Loss: 5.966265678405762, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 487/622, Lr: 0.01, Loss: 6.696975231170654, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 488/622, Lr: 0.01, Loss: 6.254123210906982, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 489/622, Lr: 0.01, Loss: 6.6630449295043945, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 490/622, Lr: 0.01, Loss: 5.282447814941406, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 491/622, Lr: 0.01, Loss: 5.856205463409424, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 492/622, Lr: 0.01, Loss: 7.09605598449707, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 493/622, Lr: 0.01, Loss: 5.480254650115967, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 494/622, Lr: 0.01, Loss: 5.635560989379883, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 495/622, Lr: 0.01, Loss: 5.591047763824463, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 496/622, Lr: 0.01, Loss: 5.131258964538574, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 497/622, Lr: 0.01, Loss: 6.3136067390441895, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 498/622, Lr: 0.01, Loss: 5.235605716705322, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 499/622, Lr: 0.01, Loss: 6.206659317016602, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 500/622, Lr: 0.01, Loss: 6.065911769866943, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 501/622, Lr: 0.01, Loss: 8.333022117614746, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 502/622, Lr: 0.01, Loss: 6.557318687438965, Hamming Loss: 0.1640625\n",
      "Epoch: 1/2, Iteration: 503/622, Lr: 0.01, Loss: 5.831264495849609, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 504/622, Lr: 0.01, Loss: 7.220493793487549, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 505/622, Lr: 0.01, Loss: 6.167781829833984, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 506/622, Lr: 0.01, Loss: 7.865825653076172, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 507/622, Lr: 0.01, Loss: 5.162416458129883, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 508/622, Lr: 0.01, Loss: 8.07266616821289, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 1/2, Iteration: 509/622, Lr: 0.01, Loss: 6.4865593910217285, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 510/622, Lr: 0.01, Loss: 6.920444488525391, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 511/622, Lr: 0.01, Loss: 3.9295289516448975, Hamming Loss: 0.140625\n",
      "Epoch: 1/2, Iteration: 512/622, Lr: 0.01, Loss: 7.151547908782959, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 513/622, Lr: 0.01, Loss: 7.543406963348389, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 514/622, Lr: 0.01, Loss: 4.734089374542236, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 515/622, Lr: 0.01, Loss: 6.397507667541504, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 516/622, Lr: 0.01, Loss: 6.457001686096191, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 517/622, Lr: 0.01, Loss: 6.535962104797363, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 518/622, Lr: 0.01, Loss: 5.860264778137207, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 519/622, Lr: 0.01, Loss: 4.9280290603637695, Hamming Loss: 0.21875\n",
      "Epoch: 1/2, Iteration: 520/622, Lr: 0.01, Loss: 4.420333385467529, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 521/622, Lr: 0.01, Loss: 7.394453048706055, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 522/622, Lr: 0.01, Loss: 6.579333305358887, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 523/622, Lr: 0.01, Loss: 5.024450302124023, Hamming Loss: 0.14583333333333334\n",
      "Epoch: 1/2, Iteration: 524/622, Lr: 0.01, Loss: 7.166980266571045, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 525/622, Lr: 0.01, Loss: 5.914926528930664, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 526/622, Lr: 0.01, Loss: 6.848834991455078, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 527/622, Lr: 0.01, Loss: 9.690113067626953, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 528/622, Lr: 0.01, Loss: 9.28031063079834, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 529/622, Lr: 0.01, Loss: 5.640692710876465, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 530/622, Lr: 0.01, Loss: 7.3229851722717285, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 531/622, Lr: 0.01, Loss: 6.968502044677734, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 532/622, Lr: 0.01, Loss: 4.2501912117004395, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 533/622, Lr: 0.01, Loss: 7.236167907714844, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 534/622, Lr: 0.01, Loss: 7.710224628448486, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 535/622, Lr: 0.01, Loss: 6.600646018981934, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 536/622, Lr: 0.01, Loss: 4.933144569396973, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 537/622, Lr: 0.01, Loss: 5.459399223327637, Hamming Loss: 0.2109375\n",
      "Epoch: 1/2, Iteration: 538/622, Lr: 0.01, Loss: 5.776674270629883, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 539/622, Lr: 0.01, Loss: 4.955799102783203, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 540/622, Lr: 0.01, Loss: 6.226580619812012, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 541/622, Lr: 0.01, Loss: 6.523162841796875, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 542/622, Lr: 0.01, Loss: 5.621767044067383, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 543/622, Lr: 0.01, Loss: 4.8583664894104, Hamming Loss: 0.125\n",
      "Epoch: 1/2, Iteration: 544/622, Lr: 0.01, Loss: 5.407426834106445, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 545/622, Lr: 0.01, Loss: 7.391218185424805, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 546/622, Lr: 0.01, Loss: 5.939818382263184, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 1/2, Iteration: 547/622, Lr: 0.01, Loss: 6.40697717666626, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 548/622, Lr: 0.01, Loss: 5.589358329772949, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 549/622, Lr: 0.01, Loss: 6.069736957550049, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 1/2, Iteration: 550/622, Lr: 0.01, Loss: 4.705667972564697, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 1/2, Iteration: 551/622, Lr: 0.01, Loss: 7.227492809295654, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 552/622, Lr: 0.01, Loss: 7.152889251708984, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 553/622, Lr: 0.01, Loss: 5.888869285583496, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 554/622, Lr: 0.01, Loss: 6.23970365524292, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 555/622, Lr: 0.01, Loss: 6.2051920890808105, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 1/2, Iteration: 556/622, Lr: 0.01, Loss: 7.685916900634766, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 557/622, Lr: 0.01, Loss: 6.096431255340576, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 558/622, Lr: 0.01, Loss: 6.576848983764648, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 559/622, Lr: 0.01, Loss: 5.955883979797363, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 560/622, Lr: 0.01, Loss: 6.150534629821777, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 1/2, Iteration: 561/622, Lr: 0.01, Loss: 7.071767330169678, Hamming Loss: 0.13541666666666666\n",
      "Epoch: 1/2, Iteration: 562/622, Lr: 0.01, Loss: 4.404385566711426, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 563/622, Lr: 0.01, Loss: 5.112966537475586, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 564/622, Lr: 0.01, Loss: 6.925533294677734, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 1/2, Iteration: 565/622, Lr: 0.01, Loss: 6.1999053955078125, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 566/622, Lr: 0.01, Loss: 5.755446434020996, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 567/622, Lr: 0.01, Loss: 7.455104351043701, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 568/622, Lr: 0.01, Loss: 5.7800374031066895, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 569/622, Lr: 0.01, Loss: 6.733027935028076, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 570/622, Lr: 0.01, Loss: 5.988213062286377, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 571/622, Lr: 0.01, Loss: 5.5754714012146, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 572/622, Lr: 0.01, Loss: 5.928182601928711, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 573/622, Lr: 0.01, Loss: 5.320513725280762, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 574/622, Lr: 0.01, Loss: 4.671276569366455, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 575/622, Lr: 0.01, Loss: 5.965387344360352, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 576/622, Lr: 0.01, Loss: 5.920327186584473, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 577/622, Lr: 0.01, Loss: 6.79176139831543, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 578/622, Lr: 0.01, Loss: 4.527615547180176, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 579/622, Lr: 0.01, Loss: 6.016655445098877, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 580/622, Lr: 0.01, Loss: 4.538362979888916, Hamming Loss: 0.14583333333333334\n",
      "Epoch: 1/2, Iteration: 581/622, Lr: 0.01, Loss: 5.501425743103027, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 582/622, Lr: 0.01, Loss: 5.143771171569824, Hamming Loss: 0.171875\n",
      "Epoch: 1/2, Iteration: 583/622, Lr: 0.01, Loss: 8.683735847473145, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 1/2, Iteration: 584/622, Lr: 0.01, Loss: 8.898719787597656, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 585/622, Lr: 0.01, Loss: 6.559370517730713, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 586/622, Lr: 0.01, Loss: 5.497830390930176, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 587/622, Lr: 0.01, Loss: 4.483809947967529, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Iteration: 588/622, Lr: 0.01, Loss: 5.715989112854004, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 589/622, Lr: 0.01, Loss: 6.614170074462891, Hamming Loss: 0.2265625\n",
      "Epoch: 1/2, Iteration: 590/622, Lr: 0.01, Loss: 6.191872596740723, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 1/2, Iteration: 591/622, Lr: 0.01, Loss: 6.656912803649902, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 592/622, Lr: 0.01, Loss: 6.493581771850586, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 593/622, Lr: 0.01, Loss: 5.339352607727051, Hamming Loss: 0.140625\n",
      "Epoch: 1/2, Iteration: 594/622, Lr: 0.01, Loss: 6.335633277893066, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 595/622, Lr: 0.01, Loss: 6.518080234527588, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 596/622, Lr: 0.01, Loss: 5.015491485595703, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 597/622, Lr: 0.01, Loss: 7.87282657623291, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 1/2, Iteration: 598/622, Lr: 0.01, Loss: 6.431992530822754, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 599/622, Lr: 0.01, Loss: 6.950728893280029, Hamming Loss: 0.1953125\n",
      "Epoch: 1/2, Iteration: 600/622, Lr: 0.01, Loss: 4.3358635902404785, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 601/622, Lr: 0.01, Loss: 4.123684883117676, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 602/622, Lr: 0.01, Loss: 5.701399803161621, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 1/2, Iteration: 603/622, Lr: 0.01, Loss: 5.647786617279053, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 1/2, Iteration: 604/622, Lr: 0.01, Loss: 7.536351203918457, Hamming Loss: 0.203125\n",
      "Epoch: 1/2, Iteration: 605/622, Lr: 0.01, Loss: 4.3122148513793945, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 1/2, Iteration: 606/622, Lr: 0.01, Loss: 4.734133720397949, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 1/2, Iteration: 607/622, Lr: 0.01, Loss: 10.85528564453125, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 1/2, Iteration: 608/622, Lr: 0.01, Loss: 7.423130989074707, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 609/622, Lr: 0.01, Loss: 5.463659286499023, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 1/2, Iteration: 610/622, Lr: 0.01, Loss: 5.9100775718688965, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 1/2, Iteration: 611/622, Lr: 0.01, Loss: 5.332302093505859, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 1/2, Iteration: 612/622, Lr: 0.01, Loss: 7.350627899169922, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 613/622, Lr: 0.01, Loss: 5.737521171569824, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 614/622, Lr: 0.01, Loss: 7.359157562255859, Hamming Loss: 0.1875\n",
      "Epoch: 1/2, Iteration: 615/622, Lr: 0.01, Loss: 5.72227144241333, Hamming Loss: 0.15625\n",
      "Epoch: 1/2, Iteration: 616/622, Lr: 0.01, Loss: 4.896759986877441, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 617/622, Lr: 0.01, Loss: 3.8753368854522705, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 618/622, Lr: 0.01, Loss: 4.8943328857421875, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 1/2, Iteration: 619/622, Lr: 0.01, Loss: 6.686123371124268, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 1/2, Iteration: 620/622, Lr: 0.01, Loss: 5.159113883972168, Hamming Loss: 0.1796875\n",
      "Epoch: 1/2, Iteration: 621/622, Lr: 0.01, Loss: 3.7857394218444824, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 1/2, Iteration: 622/622, Lr: 0.01, Loss: 5.350468158721924, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 1/2, Lr: 0.01, Loss: 6.290406227111816, Hamming Loss: 0.1773302531137003\n",
      "Epoch: 2/2, Iteration: 1/622, Lr: 0.01, Loss: 4.856048107147217, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 2/622, Lr: 0.01, Loss: 5.787858009338379, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 3/622, Lr: 0.01, Loss: 4.654992580413818, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 4/622, Lr: 0.01, Loss: 6.002616882324219, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 5/622, Lr: 0.01, Loss: 5.1390275955200195, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 6/622, Lr: 0.01, Loss: 8.093297958374023, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 7/622, Lr: 0.01, Loss: 5.809898376464844, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 8/622, Lr: 0.01, Loss: 6.856599807739258, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 9/622, Lr: 0.01, Loss: 4.261270046234131, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 10/622, Lr: 0.01, Loss: 7.3970489501953125, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 11/622, Lr: 0.01, Loss: 7.7110724449157715, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 12/622, Lr: 0.01, Loss: 5.520322322845459, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 13/622, Lr: 0.01, Loss: 5.9649977684021, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 14/622, Lr: 0.01, Loss: 5.667737007141113, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 15/622, Lr: 0.01, Loss: 6.056013107299805, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 16/622, Lr: 0.01, Loss: 6.024404048919678, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 17/622, Lr: 0.01, Loss: 4.354609966278076, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 18/622, Lr: 0.01, Loss: 6.1674485206604, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 2/2, Iteration: 19/622, Lr: 0.01, Loss: 7.206340789794922, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 20/622, Lr: 0.01, Loss: 6.467894077301025, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 21/622, Lr: 0.01, Loss: 7.122458457946777, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 22/622, Lr: 0.01, Loss: 5.55272102355957, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 23/622, Lr: 0.01, Loss: 6.550334930419922, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 24/622, Lr: 0.01, Loss: 5.434786319732666, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 25/622, Lr: 0.01, Loss: 6.270029067993164, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 26/622, Lr: 0.01, Loss: 5.888526439666748, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 27/622, Lr: 0.01, Loss: 6.624348163604736, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 28/622, Lr: 0.01, Loss: 6.249496936798096, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 29/622, Lr: 0.01, Loss: 6.343732833862305, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 30/622, Lr: 0.01, Loss: 5.664515495300293, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 31/622, Lr: 0.01, Loss: 6.228758811950684, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 32/622, Lr: 0.01, Loss: 5.017477035522461, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 33/622, Lr: 0.01, Loss: 5.72336483001709, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 34/622, Lr: 0.01, Loss: 5.629543304443359, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 35/622, Lr: 0.01, Loss: 5.699160575866699, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 36/622, Lr: 0.01, Loss: 5.899559020996094, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 37/622, Lr: 0.01, Loss: 4.892215728759766, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 38/622, Lr: 0.01, Loss: 6.899904251098633, Hamming Loss: 0.1484375\n",
      "Epoch: 2/2, Iteration: 39/622, Lr: 0.01, Loss: 6.272138595581055, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 40/622, Lr: 0.01, Loss: 6.386194229125977, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 41/622, Lr: 0.01, Loss: 4.889300346374512, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 42/622, Lr: 0.01, Loss: 4.794849395751953, Hamming Loss: 0.1484375\n",
      "Epoch: 2/2, Iteration: 43/622, Lr: 0.01, Loss: 7.203508377075195, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 44/622, Lr: 0.01, Loss: 4.459602355957031, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 45/622, Lr: 0.01, Loss: 4.55550479888916, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 46/622, Lr: 0.01, Loss: 6.823864936828613, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 47/622, Lr: 0.01, Loss: 5.631855010986328, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 48/622, Lr: 0.01, Loss: 5.569216251373291, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 49/622, Lr: 0.01, Loss: 7.65116024017334, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 50/622, Lr: 0.01, Loss: 5.348098278045654, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 51/622, Lr: 0.01, Loss: 6.97990083694458, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 52/622, Lr: 0.01, Loss: 5.42258358001709, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 53/622, Lr: 0.01, Loss: 7.285113334655762, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 54/622, Lr: 0.01, Loss: 5.265338897705078, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 55/622, Lr: 0.01, Loss: 5.766395568847656, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 56/622, Lr: 0.01, Loss: 5.37304162979126, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 57/622, Lr: 0.01, Loss: 5.751651763916016, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 58/622, Lr: 0.01, Loss: 4.946242809295654, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 59/622, Lr: 0.01, Loss: 4.8939642906188965, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 60/622, Lr: 0.01, Loss: 5.891885757446289, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 61/622, Lr: 0.01, Loss: 5.693211078643799, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 62/622, Lr: 0.01, Loss: 5.632547855377197, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 63/622, Lr: 0.01, Loss: 6.745171070098877, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 64/622, Lr: 0.01, Loss: 5.907722473144531, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 65/622, Lr: 0.01, Loss: 6.93679141998291, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 66/622, Lr: 0.01, Loss: 6.220320224761963, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 67/622, Lr: 0.01, Loss: 4.394618511199951, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 68/622, Lr: 0.01, Loss: 7.084181785583496, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 69/622, Lr: 0.01, Loss: 5.5476603507995605, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 70/622, Lr: 0.01, Loss: 6.754610061645508, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 71/622, Lr: 0.01, Loss: 6.581168174743652, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 72/622, Lr: 0.01, Loss: 7.2696709632873535, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 73/622, Lr: 0.01, Loss: 4.633128643035889, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 74/622, Lr: 0.01, Loss: 5.2199015617370605, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 75/622, Lr: 0.01, Loss: 6.2974419593811035, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 76/622, Lr: 0.01, Loss: 5.99307918548584, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 77/622, Lr: 0.01, Loss: 4.634462833404541, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 78/622, Lr: 0.01, Loss: 7.0593037605285645, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 79/622, Lr: 0.01, Loss: 4.599009037017822, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 80/622, Lr: 0.01, Loss: 4.182468414306641, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 81/622, Lr: 0.01, Loss: 8.180948257446289, Hamming Loss: 0.2265625\n",
      "Epoch: 2/2, Iteration: 82/622, Lr: 0.01, Loss: 6.568463325500488, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 83/622, Lr: 0.01, Loss: 7.1225104331970215, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 84/622, Lr: 0.01, Loss: 5.402426719665527, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 85/622, Lr: 0.01, Loss: 5.786045551300049, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 86/622, Lr: 0.01, Loss: 6.130988121032715, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 87/622, Lr: 0.01, Loss: 6.744597434997559, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 88/622, Lr: 0.01, Loss: 6.4025115966796875, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 89/622, Lr: 0.01, Loss: 5.211520671844482, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 90/622, Lr: 0.01, Loss: 6.906991004943848, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 91/622, Lr: 0.01, Loss: 5.801445960998535, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 92/622, Lr: 0.01, Loss: 7.792121887207031, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 93/622, Lr: 0.01, Loss: 6.548892498016357, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 94/622, Lr: 0.01, Loss: 5.116150379180908, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 95/622, Lr: 0.01, Loss: 5.466796875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 96/622, Lr: 0.01, Loss: 6.145806312561035, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 97/622, Lr: 0.01, Loss: 6.075031757354736, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 98/622, Lr: 0.01, Loss: 7.191126823425293, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 99/622, Lr: 0.01, Loss: 5.969071865081787, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 100/622, Lr: 0.01, Loss: 6.985320091247559, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 101/622, Lr: 0.01, Loss: 6.799283027648926, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 102/622, Lr: 0.01, Loss: 5.816402435302734, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 103/622, Lr: 0.01, Loss: 4.314732074737549, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 104/622, Lr: 0.01, Loss: 5.959834098815918, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 105/622, Lr: 0.01, Loss: 5.9026713371276855, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 106/622, Lr: 0.01, Loss: 6.060944080352783, Hamming Loss: 0.140625\n",
      "Epoch: 2/2, Iteration: 107/622, Lr: 0.01, Loss: 3.42749285697937, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 108/622, Lr: 0.01, Loss: 5.305502414703369, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 109/622, Lr: 0.01, Loss: 5.949110984802246, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 110/622, Lr: 0.01, Loss: 5.257025718688965, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 111/622, Lr: 0.01, Loss: 7.10489559173584, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 112/622, Lr: 0.01, Loss: 5.463588237762451, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 113/622, Lr: 0.01, Loss: 6.3347344398498535, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 114/622, Lr: 0.01, Loss: 6.1974334716796875, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 115/622, Lr: 0.01, Loss: 7.059160232543945, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 116/622, Lr: 0.01, Loss: 8.121198654174805, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 117/622, Lr: 0.01, Loss: 6.878870487213135, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 118/622, Lr: 0.01, Loss: 6.301088333129883, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 119/622, Lr: 0.01, Loss: 5.869743347167969, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 120/622, Lr: 0.01, Loss: 7.788359642028809, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 121/622, Lr: 0.01, Loss: 6.926728248596191, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 122/622, Lr: 0.01, Loss: 5.01380729675293, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 123/622, Lr: 0.01, Loss: 6.270547866821289, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 124/622, Lr: 0.01, Loss: 6.005593776702881, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 125/622, Lr: 0.01, Loss: 6.632391929626465, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 126/622, Lr: 0.01, Loss: 4.9087090492248535, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 127/622, Lr: 0.01, Loss: 6.398240566253662, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 128/622, Lr: 0.01, Loss: 4.279816627502441, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 129/622, Lr: 0.01, Loss: 6.7589616775512695, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 130/622, Lr: 0.01, Loss: 7.311498641967773, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 131/622, Lr: 0.01, Loss: 4.881723403930664, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 132/622, Lr: 0.01, Loss: 5.615153789520264, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 133/622, Lr: 0.01, Loss: 4.188227653503418, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 134/622, Lr: 0.01, Loss: 4.487283229827881, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 135/622, Lr: 0.01, Loss: 5.879124164581299, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 136/622, Lr: 0.01, Loss: 6.201605796813965, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 137/622, Lr: 0.01, Loss: 6.9356818199157715, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 138/622, Lr: 0.01, Loss: 5.984138488769531, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 139/622, Lr: 0.01, Loss: 7.137324333190918, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 140/622, Lr: 0.01, Loss: 5.346848487854004, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 141/622, Lr: 0.01, Loss: 5.275209903717041, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 142/622, Lr: 0.01, Loss: 6.564424991607666, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 143/622, Lr: 0.01, Loss: 6.9026288986206055, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 2/2, Iteration: 144/622, Lr: 0.01, Loss: 5.259746551513672, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 145/622, Lr: 0.01, Loss: 5.125998020172119, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 146/622, Lr: 0.01, Loss: 5.173275947570801, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 147/622, Lr: 0.01, Loss: 5.723150253295898, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 148/622, Lr: 0.01, Loss: 5.336027145385742, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 149/622, Lr: 0.01, Loss: 7.844882965087891, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 150/622, Lr: 0.01, Loss: 5.310464382171631, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 151/622, Lr: 0.01, Loss: 4.745419979095459, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 152/622, Lr: 0.01, Loss: 9.13613510131836, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 2/2, Iteration: 153/622, Lr: 0.01, Loss: 4.155065059661865, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 154/622, Lr: 0.01, Loss: 5.842838764190674, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 155/622, Lr: 0.01, Loss: 4.126378059387207, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 156/622, Lr: 0.01, Loss: 5.6894378662109375, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 157/622, Lr: 0.01, Loss: 4.997969150543213, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 158/622, Lr: 0.01, Loss: 4.407161235809326, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 159/622, Lr: 0.01, Loss: 4.462535858154297, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 160/622, Lr: 0.01, Loss: 5.637472629547119, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 161/622, Lr: 0.01, Loss: 7.364441394805908, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 162/622, Lr: 0.01, Loss: 4.14910364151001, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 163/622, Lr: 0.01, Loss: 5.877013206481934, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 164/622, Lr: 0.01, Loss: 5.713164806365967, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 165/622, Lr: 0.01, Loss: 5.591218948364258, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 166/622, Lr: 0.01, Loss: 5.153266429901123, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 167/622, Lr: 0.01, Loss: 8.23371410369873, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 168/622, Lr: 0.01, Loss: 6.335911750793457, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 169/622, Lr: 0.01, Loss: 5.9913330078125, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 170/622, Lr: 0.01, Loss: 7.980649948120117, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 171/622, Lr: 0.01, Loss: 6.421534538269043, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 172/622, Lr: 0.01, Loss: 6.340362071990967, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 173/622, Lr: 0.01, Loss: 5.683549404144287, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 174/622, Lr: 0.01, Loss: 6.176111698150635, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 175/622, Lr: 0.01, Loss: 7.037991046905518, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 176/622, Lr: 0.01, Loss: 6.24675989151001, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 177/622, Lr: 0.01, Loss: 8.285996437072754, Hamming Loss: 0.2265625\n",
      "Epoch: 2/2, Iteration: 178/622, Lr: 0.01, Loss: 5.155584812164307, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 179/622, Lr: 0.01, Loss: 6.434061050415039, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 180/622, Lr: 0.01, Loss: 6.025500297546387, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 181/622, Lr: 0.01, Loss: 6.639168739318848, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 182/622, Lr: 0.01, Loss: 6.2717108726501465, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 183/622, Lr: 0.01, Loss: 5.333552360534668, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 184/622, Lr: 0.01, Loss: 6.104613780975342, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 185/622, Lr: 0.01, Loss: 6.236016273498535, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 186/622, Lr: 0.01, Loss: 5.991680145263672, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 187/622, Lr: 0.01, Loss: 6.2342448234558105, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 188/622, Lr: 0.01, Loss: 5.051924705505371, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 189/622, Lr: 0.01, Loss: 7.166205406188965, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 190/622, Lr: 0.01, Loss: 7.0132646560668945, Hamming Loss: 0.2265625\n",
      "Epoch: 2/2, Iteration: 191/622, Lr: 0.01, Loss: 6.110545635223389, Hamming Loss: 0.234375\n",
      "Epoch: 2/2, Iteration: 192/622, Lr: 0.01, Loss: 5.211082458496094, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 193/622, Lr: 0.01, Loss: 6.438222885131836, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 194/622, Lr: 0.01, Loss: 4.441307067871094, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 195/622, Lr: 0.01, Loss: 6.160564422607422, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 196/622, Lr: 0.01, Loss: 5.608975887298584, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 197/622, Lr: 0.01, Loss: 7.725647926330566, Hamming Loss: 0.2265625\n",
      "Epoch: 2/2, Iteration: 198/622, Lr: 0.01, Loss: 4.409741401672363, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 199/622, Lr: 0.01, Loss: 6.416818618774414, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 200/622, Lr: 0.01, Loss: 6.0681304931640625, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 201/622, Lr: 0.01, Loss: 5.901005744934082, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 202/622, Lr: 0.01, Loss: 7.144231796264648, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 203/622, Lr: 0.01, Loss: 5.560545444488525, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 204/622, Lr: 0.01, Loss: 5.923017501831055, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 205/622, Lr: 0.01, Loss: 5.25278377532959, Hamming Loss: 0.1484375\n",
      "Epoch: 2/2, Iteration: 206/622, Lr: 0.01, Loss: 6.089502334594727, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 207/622, Lr: 0.01, Loss: 5.515371322631836, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 208/622, Lr: 0.01, Loss: 7.075488090515137, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 209/622, Lr: 0.01, Loss: 6.595876693725586, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 210/622, Lr: 0.01, Loss: 6.343195915222168, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 211/622, Lr: 0.01, Loss: 4.97725772857666, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 212/622, Lr: 0.01, Loss: 6.065896034240723, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 213/622, Lr: 0.01, Loss: 8.54634952545166, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 214/622, Lr: 0.01, Loss: 5.277523517608643, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 215/622, Lr: 0.01, Loss: 6.986941814422607, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 216/622, Lr: 0.01, Loss: 7.047509670257568, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 217/622, Lr: 0.01, Loss: 5.47687292098999, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 218/622, Lr: 0.01, Loss: 7.0442585945129395, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 219/622, Lr: 0.01, Loss: 4.587455749511719, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 220/622, Lr: 0.01, Loss: 4.822605609893799, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 221/622, Lr: 0.01, Loss: 5.43681526184082, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 222/622, Lr: 0.01, Loss: 6.966431140899658, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 223/622, Lr: 0.01, Loss: 4.979528427124023, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 224/622, Lr: 0.01, Loss: 5.113823413848877, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 225/622, Lr: 0.01, Loss: 5.60212516784668, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 226/622, Lr: 0.01, Loss: 7.928092956542969, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 227/622, Lr: 0.01, Loss: 7.223768711090088, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 228/622, Lr: 0.01, Loss: 5.635833263397217, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 229/622, Lr: 0.01, Loss: 6.116969585418701, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 230/622, Lr: 0.01, Loss: 8.975496292114258, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 231/622, Lr: 0.01, Loss: 6.974118232727051, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 232/622, Lr: 0.01, Loss: 6.679883003234863, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 233/622, Lr: 0.01, Loss: 3.7717337608337402, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 234/622, Lr: 0.01, Loss: 9.041105270385742, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 235/622, Lr: 0.01, Loss: 5.952579021453857, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 236/622, Lr: 0.01, Loss: 6.849710464477539, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 237/622, Lr: 0.01, Loss: 4.5341997146606445, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 238/622, Lr: 0.01, Loss: 6.267378807067871, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 239/622, Lr: 0.01, Loss: 6.768601894378662, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 240/622, Lr: 0.01, Loss: 6.967689037322998, Hamming Loss: 0.234375\n",
      "Epoch: 2/2, Iteration: 241/622, Lr: 0.01, Loss: 5.5943098068237305, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 242/622, Lr: 0.01, Loss: 5.627213954925537, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 243/622, Lr: 0.01, Loss: 5.444297790527344, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 244/622, Lr: 0.01, Loss: 4.292434215545654, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 245/622, Lr: 0.01, Loss: 4.326092720031738, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 246/622, Lr: 0.01, Loss: 6.745981693267822, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 247/622, Lr: 0.01, Loss: 5.556160926818848, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 248/622, Lr: 0.01, Loss: 4.1940836906433105, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 249/622, Lr: 0.01, Loss: 5.728720664978027, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 250/622, Lr: 0.01, Loss: 6.3906450271606445, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 251/622, Lr: 0.01, Loss: 6.700565338134766, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 252/622, Lr: 0.01, Loss: 6.3814697265625, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 253/622, Lr: 0.01, Loss: 7.589311122894287, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 254/622, Lr: 0.01, Loss: 5.9406609535217285, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 255/622, Lr: 0.01, Loss: 6.466134071350098, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 256/622, Lr: 0.01, Loss: 5.210023403167725, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 257/622, Lr: 0.01, Loss: 5.235410690307617, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 258/622, Lr: 0.01, Loss: 4.31721830368042, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 259/622, Lr: 0.01, Loss: 7.0383806228637695, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 260/622, Lr: 0.01, Loss: 5.4291276931762695, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 261/622, Lr: 0.01, Loss: 6.7614030838012695, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 262/622, Lr: 0.01, Loss: 7.849961280822754, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 263/622, Lr: 0.01, Loss: 6.038562774658203, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 264/622, Lr: 0.01, Loss: 5.159065246582031, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 265/622, Lr: 0.01, Loss: 7.683309555053711, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 266/622, Lr: 0.01, Loss: 6.117579460144043, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 267/622, Lr: 0.01, Loss: 6.479131698608398, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 268/622, Lr: 0.01, Loss: 5.5903849601745605, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 269/622, Lr: 0.01, Loss: 6.744437217712402, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 270/622, Lr: 0.01, Loss: 5.446289539337158, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 271/622, Lr: 0.01, Loss: 7.207787036895752, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 272/622, Lr: 0.01, Loss: 6.211324691772461, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 273/622, Lr: 0.01, Loss: 6.6396355628967285, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 274/622, Lr: 0.01, Loss: 6.23588752746582, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 275/622, Lr: 0.01, Loss: 4.7870330810546875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 276/622, Lr: 0.01, Loss: 4.728192329406738, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 277/622, Lr: 0.01, Loss: 6.822011470794678, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 278/622, Lr: 0.01, Loss: 6.930948734283447, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 279/622, Lr: 0.01, Loss: 6.13399600982666, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 280/622, Lr: 0.01, Loss: 4.996524333953857, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 281/622, Lr: 0.01, Loss: 7.403163433074951, Hamming Loss: 0.23697916666666666\n",
      "Epoch: 2/2, Iteration: 282/622, Lr: 0.01, Loss: 4.013381481170654, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 283/622, Lr: 0.01, Loss: 4.901708602905273, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 284/622, Lr: 0.01, Loss: 5.2992448806762695, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 285/622, Lr: 0.01, Loss: 6.700809955596924, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 286/622, Lr: 0.01, Loss: 5.522690773010254, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 287/622, Lr: 0.01, Loss: 6.762070655822754, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 2/2, Iteration: 288/622, Lr: 0.01, Loss: 6.26833438873291, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 289/622, Lr: 0.01, Loss: 6.884783744812012, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 290/622, Lr: 0.01, Loss: 6.6502461433410645, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 291/622, Lr: 0.01, Loss: 6.478824615478516, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 292/622, Lr: 0.01, Loss: 5.073923587799072, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 293/622, Lr: 0.01, Loss: 6.669991493225098, Hamming Loss: 0.140625\n",
      "Epoch: 2/2, Iteration: 294/622, Lr: 0.01, Loss: 8.251474380493164, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 295/622, Lr: 0.01, Loss: 5.5373735427856445, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 296/622, Lr: 0.01, Loss: 5.849481105804443, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 297/622, Lr: 0.01, Loss: 5.800095558166504, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 298/622, Lr: 0.01, Loss: 3.956333637237549, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 299/622, Lr: 0.01, Loss: 7.190757751464844, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 300/622, Lr: 0.01, Loss: 6.500641822814941, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 301/622, Lr: 0.01, Loss: 6.460179328918457, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 302/622, Lr: 0.01, Loss: 5.014258861541748, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 303/622, Lr: 0.01, Loss: 4.939357280731201, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 304/622, Lr: 0.01, Loss: 5.5917463302612305, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 305/622, Lr: 0.01, Loss: 6.149946212768555, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 306/622, Lr: 0.01, Loss: 4.98880672454834, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 307/622, Lr: 0.01, Loss: 6.187992572784424, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 308/622, Lr: 0.01, Loss: 4.313364028930664, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 309/622, Lr: 0.01, Loss: 5.282581806182861, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 310/622, Lr: 0.01, Loss: 5.027047634124756, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 311/622, Lr: 0.01, Loss: 4.864054203033447, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 312/622, Lr: 0.01, Loss: 6.5518317222595215, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 313/622, Lr: 0.01, Loss: 5.69158935546875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 314/622, Lr: 0.01, Loss: 5.4119648933410645, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 315/622, Lr: 0.01, Loss: 10.901026725769043, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 316/622, Lr: 0.01, Loss: 5.958691120147705, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 317/622, Lr: 0.01, Loss: 5.912590980529785, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 318/622, Lr: 0.01, Loss: 5.679595470428467, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 319/622, Lr: 0.01, Loss: 5.410647392272949, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 320/622, Lr: 0.01, Loss: 4.658884048461914, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 321/622, Lr: 0.01, Loss: 6.535574913024902, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 322/622, Lr: 0.01, Loss: 6.934800148010254, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 323/622, Lr: 0.01, Loss: 4.344727039337158, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 324/622, Lr: 0.01, Loss: 5.267611980438232, Hamming Loss: 0.1328125\n",
      "Epoch: 2/2, Iteration: 325/622, Lr: 0.01, Loss: 8.554841041564941, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 326/622, Lr: 0.01, Loss: 5.3454670906066895, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 327/622, Lr: 0.01, Loss: 5.002451419830322, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 328/622, Lr: 0.01, Loss: 6.093615531921387, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 329/622, Lr: 0.01, Loss: 7.467475891113281, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 330/622, Lr: 0.01, Loss: 6.533984661102295, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 331/622, Lr: 0.01, Loss: 6.838745594024658, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 2/2, Iteration: 332/622, Lr: 0.01, Loss: 5.0028886795043945, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 333/622, Lr: 0.01, Loss: 4.808412075042725, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 334/622, Lr: 0.01, Loss: 6.355281352996826, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 335/622, Lr: 0.01, Loss: 5.533634185791016, Hamming Loss: 0.234375\n",
      "Epoch: 2/2, Iteration: 336/622, Lr: 0.01, Loss: 6.042194366455078, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 337/622, Lr: 0.01, Loss: 5.838814735412598, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 338/622, Lr: 0.01, Loss: 5.294793128967285, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 339/622, Lr: 0.01, Loss: 7.785125732421875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 340/622, Lr: 0.01, Loss: 6.637939453125, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 341/622, Lr: 0.01, Loss: 4.775198459625244, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 342/622, Lr: 0.01, Loss: 5.849670886993408, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 343/622, Lr: 0.01, Loss: 5.690515995025635, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 344/622, Lr: 0.01, Loss: 5.110873222351074, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 345/622, Lr: 0.01, Loss: 7.0013532638549805, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 346/622, Lr: 0.01, Loss: 4.196671962738037, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 347/622, Lr: 0.01, Loss: 5.336497783660889, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 348/622, Lr: 0.01, Loss: 4.518113136291504, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 349/622, Lr: 0.01, Loss: 6.401769161224365, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 350/622, Lr: 0.01, Loss: 5.786164283752441, Hamming Loss: 0.13802083333333334\n",
      "Epoch: 2/2, Iteration: 351/622, Lr: 0.01, Loss: 4.331770420074463, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 352/622, Lr: 0.01, Loss: 8.189573287963867, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 353/622, Lr: 0.01, Loss: 7.700903415679932, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 354/622, Lr: 0.01, Loss: 3.752732515335083, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 355/622, Lr: 0.01, Loss: 4.782851219177246, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 356/622, Lr: 0.01, Loss: 5.751534461975098, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 357/622, Lr: 0.01, Loss: 7.131422519683838, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 358/622, Lr: 0.01, Loss: 5.930259704589844, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 359/622, Lr: 0.01, Loss: 6.231834411621094, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 360/622, Lr: 0.01, Loss: 6.743692398071289, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 361/622, Lr: 0.01, Loss: 5.926079750061035, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 362/622, Lr: 0.01, Loss: 5.857521057128906, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 363/622, Lr: 0.01, Loss: 4.65683126449585, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 364/622, Lr: 0.01, Loss: 5.8185601234436035, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 365/622, Lr: 0.01, Loss: 5.448010444641113, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 366/622, Lr: 0.01, Loss: 5.530979156494141, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 367/622, Lr: 0.01, Loss: 5.356307506561279, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 368/622, Lr: 0.01, Loss: 5.586711883544922, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 369/622, Lr: 0.01, Loss: 4.740668773651123, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 370/622, Lr: 0.01, Loss: 5.634842395782471, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 371/622, Lr: 0.01, Loss: 5.453442573547363, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 372/622, Lr: 0.01, Loss: 4.739287853240967, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 2/2, Iteration: 373/622, Lr: 0.01, Loss: 6.12713098526001, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 374/622, Lr: 0.01, Loss: 8.687965393066406, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 375/622, Lr: 0.01, Loss: 6.822702884674072, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 376/622, Lr: 0.01, Loss: 5.947464942932129, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 377/622, Lr: 0.01, Loss: 5.845254421234131, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 378/622, Lr: 0.01, Loss: 6.365846633911133, Hamming Loss: 0.21614583333333334\n",
      "Epoch: 2/2, Iteration: 379/622, Lr: 0.01, Loss: 6.584822654724121, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 380/622, Lr: 0.01, Loss: 4.633784294128418, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 381/622, Lr: 0.01, Loss: 5.066652297973633, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 382/622, Lr: 0.01, Loss: 4.635791778564453, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 383/622, Lr: 0.01, Loss: 4.643110752105713, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 384/622, Lr: 0.01, Loss: 5.539055347442627, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 385/622, Lr: 0.01, Loss: 6.065445899963379, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 386/622, Lr: 0.01, Loss: 5.6485161781311035, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 387/622, Lr: 0.01, Loss: 7.476139068603516, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 388/622, Lr: 0.01, Loss: 6.674563884735107, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 389/622, Lr: 0.01, Loss: 6.474278450012207, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 390/622, Lr: 0.01, Loss: 6.7094035148620605, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 391/622, Lr: 0.01, Loss: 7.695034980773926, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 392/622, Lr: 0.01, Loss: 3.697187900543213, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 393/622, Lr: 0.01, Loss: 5.8633131980896, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 394/622, Lr: 0.01, Loss: 5.117839336395264, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 395/622, Lr: 0.01, Loss: 5.4249773025512695, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 396/622, Lr: 0.01, Loss: 5.203117847442627, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 397/622, Lr: 0.01, Loss: 6.396111488342285, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 398/622, Lr: 0.01, Loss: 5.66546106338501, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 399/622, Lr: 0.01, Loss: 7.709087371826172, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 400/622, Lr: 0.01, Loss: 6.8037428855896, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 401/622, Lr: 0.01, Loss: 8.365116119384766, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 402/622, Lr: 0.01, Loss: 5.946341514587402, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 403/622, Lr: 0.01, Loss: 5.42360258102417, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 404/622, Lr: 0.01, Loss: 7.124511241912842, Hamming Loss: 0.22135416666666666\n",
      "Epoch: 2/2, Iteration: 405/622, Lr: 0.01, Loss: 3.877251386642456, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 406/622, Lr: 0.01, Loss: 5.697542190551758, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 407/622, Lr: 0.01, Loss: 7.029986381530762, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 408/622, Lr: 0.01, Loss: 5.470248699188232, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 409/622, Lr: 0.01, Loss: 13.43934154510498, Hamming Loss: 0.2265625\n",
      "Epoch: 2/2, Iteration: 410/622, Lr: 0.01, Loss: 5.043677806854248, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 411/622, Lr: 0.01, Loss: 7.165060520172119, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 412/622, Lr: 0.01, Loss: 6.769041538238525, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 413/622, Lr: 0.01, Loss: 6.863924980163574, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 414/622, Lr: 0.01, Loss: 5.759738922119141, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 415/622, Lr: 0.01, Loss: 5.972447395324707, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 416/622, Lr: 0.01, Loss: 4.383177280426025, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 417/622, Lr: 0.01, Loss: 3.482879638671875, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 418/622, Lr: 0.01, Loss: 5.758024215698242, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 419/622, Lr: 0.01, Loss: 5.102135181427002, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 420/622, Lr: 0.01, Loss: 3.060758113861084, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 421/622, Lr: 0.01, Loss: 4.6473469734191895, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 422/622, Lr: 0.01, Loss: 6.159850120544434, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 423/622, Lr: 0.01, Loss: 5.4323225021362305, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 424/622, Lr: 0.01, Loss: 5.545899868011475, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 425/622, Lr: 0.01, Loss: 4.709116458892822, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 426/622, Lr: 0.01, Loss: 7.607766628265381, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 427/622, Lr: 0.01, Loss: 6.338336944580078, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 428/622, Lr: 0.01, Loss: 4.643589019775391, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 429/622, Lr: 0.01, Loss: 6.260439872741699, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 430/622, Lr: 0.01, Loss: 5.716372489929199, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 431/622, Lr: 0.01, Loss: 4.07406759262085, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 432/622, Lr: 0.01, Loss: 6.5875983238220215, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 433/622, Lr: 0.01, Loss: 5.182409763336182, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 434/622, Lr: 0.01, Loss: 6.029181480407715, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 435/622, Lr: 0.01, Loss: 6.916073322296143, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 436/622, Lr: 0.01, Loss: 5.376772403717041, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 437/622, Lr: 0.01, Loss: 6.686059951782227, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 438/622, Lr: 0.01, Loss: 5.416787624359131, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 439/622, Lr: 0.01, Loss: 5.535086631774902, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 440/622, Lr: 0.01, Loss: 7.736428260803223, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 441/622, Lr: 0.01, Loss: 8.119982719421387, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 442/622, Lr: 0.01, Loss: 5.23456335067749, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 443/622, Lr: 0.01, Loss: 5.06816291809082, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 444/622, Lr: 0.01, Loss: 6.730067729949951, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 445/622, Lr: 0.01, Loss: 7.742549896240234, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 446/622, Lr: 0.01, Loss: 7.788055419921875, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 447/622, Lr: 0.01, Loss: 4.896299362182617, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 448/622, Lr: 0.01, Loss: 6.397629261016846, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 449/622, Lr: 0.01, Loss: 6.279006481170654, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 450/622, Lr: 0.01, Loss: 5.894472599029541, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 451/622, Lr: 0.01, Loss: 5.291916847229004, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 452/622, Lr: 0.01, Loss: 6.784743309020996, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 453/622, Lr: 0.01, Loss: 4.313939571380615, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 454/622, Lr: 0.01, Loss: 5.843513488769531, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 455/622, Lr: 0.01, Loss: 4.763937473297119, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 456/622, Lr: 0.01, Loss: 3.876842975616455, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 457/622, Lr: 0.01, Loss: 5.655575752258301, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 458/622, Lr: 0.01, Loss: 6.564671516418457, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 459/622, Lr: 0.01, Loss: 5.915678024291992, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 460/622, Lr: 0.01, Loss: 7.313241958618164, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 461/622, Lr: 0.01, Loss: 6.672105312347412, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 462/622, Lr: 0.01, Loss: 4.40331506729126, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 463/622, Lr: 0.01, Loss: 5.825743675231934, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 464/622, Lr: 0.01, Loss: 4.950037002563477, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 2/2, Iteration: 465/622, Lr: 0.01, Loss: 6.386995315551758, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 466/622, Lr: 0.01, Loss: 4.65886116027832, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 467/622, Lr: 0.01, Loss: 6.242491245269775, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 468/622, Lr: 0.01, Loss: 7.098720550537109, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 469/622, Lr: 0.01, Loss: 3.2370495796203613, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 470/622, Lr: 0.01, Loss: 7.898695945739746, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 471/622, Lr: 0.01, Loss: 5.186466217041016, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 472/622, Lr: 0.01, Loss: 5.4679484367370605, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 473/622, Lr: 0.01, Loss: 6.41899299621582, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 474/622, Lr: 0.01, Loss: 4.808886528015137, Hamming Loss: 0.14322916666666666\n",
      "Epoch: 2/2, Iteration: 475/622, Lr: 0.01, Loss: 5.586181163787842, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 476/622, Lr: 0.01, Loss: 6.041647911071777, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 477/622, Lr: 0.01, Loss: 5.047962665557861, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 478/622, Lr: 0.01, Loss: 3.476163864135742, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 479/622, Lr: 0.01, Loss: 6.680414199829102, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 480/622, Lr: 0.01, Loss: 5.681691646575928, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 481/622, Lr: 0.01, Loss: 5.239630699157715, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 482/622, Lr: 0.01, Loss: 7.9332499504089355, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 483/622, Lr: 0.01, Loss: 6.374203205108643, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 484/622, Lr: 0.01, Loss: 7.100824356079102, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 485/622, Lr: 0.01, Loss: 4.46467399597168, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 486/622, Lr: 0.01, Loss: 5.7764177322387695, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 487/622, Lr: 0.01, Loss: 8.191120147705078, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 488/622, Lr: 0.01, Loss: 6.0477681159973145, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 489/622, Lr: 0.01, Loss: 5.62135124206543, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 490/622, Lr: 0.01, Loss: 5.063751220703125, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 491/622, Lr: 0.01, Loss: 6.38553524017334, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 492/622, Lr: 0.01, Loss: 6.627221584320068, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 493/622, Lr: 0.01, Loss: 6.010778903961182, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 494/622, Lr: 0.01, Loss: 5.667122840881348, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 495/622, Lr: 0.01, Loss: 5.067124366760254, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 496/622, Lr: 0.01, Loss: 5.791842460632324, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 497/622, Lr: 0.01, Loss: 5.83355188369751, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 498/622, Lr: 0.01, Loss: 5.488412380218506, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 499/622, Lr: 0.01, Loss: 6.112534523010254, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 500/622, Lr: 0.01, Loss: 5.686775207519531, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 501/622, Lr: 0.01, Loss: 7.262028694152832, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 502/622, Lr: 0.01, Loss: 9.734285354614258, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 503/622, Lr: 0.01, Loss: 5.225494384765625, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 504/622, Lr: 0.01, Loss: 6.382990837097168, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 505/622, Lr: 0.01, Loss: 5.505177021026611, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 506/622, Lr: 0.01, Loss: 4.727632522583008, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 507/622, Lr: 0.01, Loss: 7.337831497192383, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 508/622, Lr: 0.01, Loss: 5.652499198913574, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 509/622, Lr: 0.01, Loss: 5.665017127990723, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 510/622, Lr: 0.01, Loss: 5.241225242614746, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 511/622, Lr: 0.01, Loss: 5.255122184753418, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 512/622, Lr: 0.01, Loss: 5.486293792724609, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 513/622, Lr: 0.01, Loss: 6.142399787902832, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 514/622, Lr: 0.01, Loss: 5.05977201461792, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 515/622, Lr: 0.01, Loss: 9.418084144592285, Hamming Loss: 0.23177083333333334\n",
      "Epoch: 2/2, Iteration: 516/622, Lr: 0.01, Loss: 4.21209192276001, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 517/622, Lr: 0.01, Loss: 6.319465637207031, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 518/622, Lr: 0.01, Loss: 6.231895923614502, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 519/622, Lr: 0.01, Loss: 5.5466694831848145, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 520/622, Lr: 0.01, Loss: 5.202611446380615, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 521/622, Lr: 0.01, Loss: 5.282237529754639, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 522/622, Lr: 0.01, Loss: 6.3435797691345215, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 523/622, Lr: 0.01, Loss: 7.169804096221924, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 524/622, Lr: 0.01, Loss: 4.700397491455078, Hamming Loss: 0.21875\n",
      "Epoch: 2/2, Iteration: 525/622, Lr: 0.01, Loss: 5.7104902267456055, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 526/622, Lr: 0.01, Loss: 5.7608489990234375, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 527/622, Lr: 0.01, Loss: 6.321177959442139, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 528/622, Lr: 0.01, Loss: 5.594642162322998, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 529/622, Lr: 0.01, Loss: 6.869800090789795, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 530/622, Lr: 0.01, Loss: 8.6298189163208, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 531/622, Lr: 0.01, Loss: 5.090944766998291, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 532/622, Lr: 0.01, Loss: 6.002341270446777, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 533/622, Lr: 0.01, Loss: 6.208682060241699, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 534/622, Lr: 0.01, Loss: 5.332598686218262, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 2/2, Iteration: 535/622, Lr: 0.01, Loss: 4.678177356719971, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 536/622, Lr: 0.01, Loss: 4.89868688583374, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 537/622, Lr: 0.01, Loss: 6.668125152587891, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 538/622, Lr: 0.01, Loss: 5.062953948974609, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 539/622, Lr: 0.01, Loss: 6.334742069244385, Hamming Loss: 0.2109375\n",
      "Epoch: 2/2, Iteration: 540/622, Lr: 0.01, Loss: 5.42425012588501, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 541/622, Lr: 0.01, Loss: 5.044975280761719, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 542/622, Lr: 0.01, Loss: 7.375373840332031, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 543/622, Lr: 0.01, Loss: 4.021596908569336, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 544/622, Lr: 0.01, Loss: 4.759127616882324, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 545/622, Lr: 0.01, Loss: 4.841341495513916, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 546/622, Lr: 0.01, Loss: 5.5124125480651855, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 547/622, Lr: 0.01, Loss: 8.250716209411621, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 548/622, Lr: 0.01, Loss: 4.401518821716309, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 549/622, Lr: 0.01, Loss: 6.350447177886963, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 550/622, Lr: 0.01, Loss: 5.053627014160156, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 551/622, Lr: 0.01, Loss: 5.51335334777832, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 552/622, Lr: 0.01, Loss: 4.91843843460083, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 553/622, Lr: 0.01, Loss: 5.345645904541016, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 554/622, Lr: 0.01, Loss: 7.463809013366699, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 555/622, Lr: 0.01, Loss: 7.151204586029053, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 556/622, Lr: 0.01, Loss: 5.708698272705078, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 557/622, Lr: 0.01, Loss: 6.707488536834717, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 558/622, Lr: 0.01, Loss: 5.025510787963867, Hamming Loss: 0.13541666666666666\n",
      "Epoch: 2/2, Iteration: 559/622, Lr: 0.01, Loss: 6.899294853210449, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 560/622, Lr: 0.01, Loss: 7.5152411460876465, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 561/622, Lr: 0.01, Loss: 4.753314971923828, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 562/622, Lr: 0.01, Loss: 4.238960266113281, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 563/622, Lr: 0.01, Loss: 5.864321231842041, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 564/622, Lr: 0.01, Loss: 7.777846336364746, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 565/622, Lr: 0.01, Loss: 8.493646621704102, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 566/622, Lr: 0.01, Loss: 7.178951740264893, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 567/622, Lr: 0.01, Loss: 5.5344767570495605, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 568/622, Lr: 0.01, Loss: 4.6120924949646, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 569/622, Lr: 0.01, Loss: 5.855182647705078, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 570/622, Lr: 0.01, Loss: 7.324921131134033, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 571/622, Lr: 0.01, Loss: 5.67201566696167, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 572/622, Lr: 0.01, Loss: 6.491219997406006, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 573/622, Lr: 0.01, Loss: 4.524310111999512, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 574/622, Lr: 0.01, Loss: 7.179368495941162, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 575/622, Lr: 0.01, Loss: 4.25654935836792, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 576/622, Lr: 0.01, Loss: 4.242407321929932, Hamming Loss: 0.16927083333333334\n",
      "Epoch: 2/2, Iteration: 577/622, Lr: 0.01, Loss: 5.049251556396484, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 578/622, Lr: 0.01, Loss: 6.281947135925293, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 579/622, Lr: 0.01, Loss: 6.159974098205566, Hamming Loss: 0.1640625\n",
      "Epoch: 2/2, Iteration: 580/622, Lr: 0.01, Loss: 5.696938514709473, Hamming Loss: 0.17708333333333334\n",
      "Epoch: 2/2, Iteration: 581/622, Lr: 0.01, Loss: 5.453104496002197, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 582/622, Lr: 0.01, Loss: 5.434283256530762, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 583/622, Lr: 0.01, Loss: 4.120968341827393, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 584/622, Lr: 0.01, Loss: 5.885325908660889, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 585/622, Lr: 0.01, Loss: 6.995046138763428, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 586/622, Lr: 0.01, Loss: 5.840762615203857, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 587/622, Lr: 0.01, Loss: 5.296344757080078, Hamming Loss: 0.20052083333333334\n",
      "Epoch: 2/2, Iteration: 588/622, Lr: 0.01, Loss: 6.410750389099121, Hamming Loss: 0.15885416666666666\n",
      "Epoch: 2/2, Iteration: 589/622, Lr: 0.01, Loss: 6.987061500549316, Hamming Loss: 0.18229166666666666\n",
      "Epoch: 2/2, Iteration: 590/622, Lr: 0.01, Loss: 6.8566999435424805, Hamming Loss: 0.20833333333333334\n",
      "Epoch: 2/2, Iteration: 591/622, Lr: 0.01, Loss: 3.6336569786071777, Hamming Loss: 0.15104166666666666\n",
      "Epoch: 2/2, Iteration: 592/622, Lr: 0.01, Loss: 5.632379055023193, Hamming Loss: 0.1796875\n",
      "Epoch: 2/2, Iteration: 593/622, Lr: 0.01, Loss: 5.974984645843506, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 594/622, Lr: 0.01, Loss: 5.931434631347656, Hamming Loss: 0.1484375\n",
      "Epoch: 2/2, Iteration: 595/622, Lr: 0.01, Loss: 4.667098522186279, Hamming Loss: 0.16666666666666666\n",
      "Epoch: 2/2, Iteration: 596/622, Lr: 0.01, Loss: 5.5906877517700195, Hamming Loss: 0.22395833333333334\n",
      "Epoch: 2/2, Iteration: 597/622, Lr: 0.01, Loss: 4.359212875366211, Hamming Loss: 0.15625\n",
      "Epoch: 2/2, Iteration: 598/622, Lr: 0.01, Loss: 6.321691989898682, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 599/622, Lr: 0.01, Loss: 6.252608299255371, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 600/622, Lr: 0.01, Loss: 6.731841087341309, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 2/2, Iteration: 601/622, Lr: 0.01, Loss: 5.866552352905273, Hamming Loss: 0.19270833333333334\n",
      "Epoch: 2/2, Iteration: 602/622, Lr: 0.01, Loss: 4.601080894470215, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 603/622, Lr: 0.01, Loss: 6.181002140045166, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 604/622, Lr: 0.01, Loss: 8.439571380615234, Hamming Loss: 0.203125\n",
      "Epoch: 2/2, Iteration: 605/622, Lr: 0.01, Loss: 6.2684245109558105, Hamming Loss: 0.20572916666666666\n",
      "Epoch: 2/2, Iteration: 606/622, Lr: 0.01, Loss: 4.3635382652282715, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 607/622, Lr: 0.01, Loss: 6.469611167907715, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 608/622, Lr: 0.01, Loss: 6.411341667175293, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 609/622, Lr: 0.01, Loss: 6.454955101013184, Hamming Loss: 0.22916666666666666\n",
      "Epoch: 2/2, Iteration: 610/622, Lr: 0.01, Loss: 6.647252082824707, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 611/622, Lr: 0.01, Loss: 7.740642547607422, Hamming Loss: 0.19791666666666666\n",
      "Epoch: 2/2, Iteration: 612/622, Lr: 0.01, Loss: 5.446437835693359, Hamming Loss: 0.1875\n",
      "Epoch: 2/2, Iteration: 613/622, Lr: 0.01, Loss: 6.664033889770508, Hamming Loss: 0.21354166666666666\n",
      "Epoch: 2/2, Iteration: 614/622, Lr: 0.01, Loss: 7.244867324829102, Hamming Loss: 0.19010416666666666\n",
      "Epoch: 2/2, Iteration: 615/622, Lr: 0.01, Loss: 4.1346025466918945, Hamming Loss: 0.16145833333333334\n",
      "Epoch: 2/2, Iteration: 616/622, Lr: 0.01, Loss: 5.4766459465026855, Hamming Loss: 0.171875\n",
      "Epoch: 2/2, Iteration: 617/622, Lr: 0.01, Loss: 5.413397312164307, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 618/622, Lr: 0.01, Loss: 6.586780548095703, Hamming Loss: 0.15364583333333334\n",
      "Epoch: 2/2, Iteration: 619/622, Lr: 0.01, Loss: 5.615901947021484, Hamming Loss: 0.17447916666666666\n",
      "Epoch: 2/2, Iteration: 620/622, Lr: 0.01, Loss: 7.323899269104004, Hamming Loss: 0.18489583333333334\n",
      "Epoch: 2/2, Iteration: 621/622, Lr: 0.01, Loss: 5.681453227996826, Hamming Loss: 0.1953125\n",
      "Epoch: 2/2, Iteration: 622/622, Lr: 0.01, Loss: 8.554986953735352, Hamming Loss: 0.21875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for iter, (feature, label) in enumerate(training_generator):\n",
    "        if torch.cuda.is_available():\n",
    "            feature = feature.cuda()\n",
    "            label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model._init_hidden_state()\n",
    "        predictions = model(feature)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_metrics = get_modified_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"hamming_loss\"], threshold=0.25)\n",
    "        print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Hamming Loss: {}\".format(\n",
    "            epoch + 1,\n",
    "            num_epoches,\n",
    "            iter + 1,\n",
    "            num_iter_per_epoch,\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            loss, training_metrics[\"hamming_loss\"]))\n",
    "        # writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
    "    if epoch % test_interval == 0:\n",
    "        model.eval()\n",
    "        loss_ls = []\n",
    "        te_label_ls = []\n",
    "        te_pred_ls = []\n",
    "        for te_feature, te_label in test_generator:\n",
    "            num_sample = len(te_label)\n",
    "            if torch.cuda.is_available():\n",
    "                te_feature = te_feature.cuda()\n",
    "                te_label = te_label.cuda()\n",
    "            with torch.no_grad():\n",
    "                model._init_hidden_state(num_sample)\n",
    "                te_predictions = model(te_feature)\n",
    "            te_loss = criterion(te_predictions, te_label)\n",
    "            loss_ls.append(te_loss * num_sample)\n",
    "            te_label_ls.extend(te_label.clone().cpu())\n",
    "            te_pred_ls.append(te_predictions.clone().cpu())\n",
    "        te_loss = sum(loss_ls) / test_set.__len__()\n",
    "        te_pred = torch.cat(te_pred_ls, 0)\n",
    "        te_label = np.array([te_label_ten.numpy() for te_label_ten in te_label_ls])\n",
    "        test_metrics = get_modified_evaluation(te_label, te_pred.numpy(), list_metrics=[\"hamming_loss\"], threshold=0.25) \n",
    "        # test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"loss\"]) #list_metrics=[\"accuracy\", \"confusion_matrix\"]\n",
    "        # output_file.write(\n",
    "        #     \"Epoch: {}/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n\".format(\n",
    "        #         epoch + 1, num_epoches,\n",
    "        #         te_loss,\n",
    "        #         test_metrics[\"accuracy\"],\n",
    "        #         test_metrics[\"confusion_matrix\"]))\n",
    "        print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Hamming Loss: {}\".format(\n",
    "            epoch + 1,\n",
    "            num_epoches,\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            te_loss, test_metrics[\"hamming_loss\"]))\n",
    "        # writer.add_scalar('Test/Loss', te_loss, epoch)\n",
    "        # writer.add_scalar('Test/Accuracy', test_metrics[\"accuracy\"], epoch)\n",
    "        model.train()\n",
    "        # if te_loss + es_min_delta < best_loss:\n",
    "        #     best_loss = te_loss\n",
    "        #     best_epoch = epoch\n",
    "        #     torch.save(model, saved_path + os.sep + \"whole_model_han\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch > es_patience > 0:\n",
    "            print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
