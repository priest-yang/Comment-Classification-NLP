{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import matrix_mul, element_wise_mul\n",
    "\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # Gets the current notebook directory\n",
    "src_dir = os.path.join(cur_dir, '../src')  # Constructs the path to the 'src' directory\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n",
    "        super(MyDataset, self).__init__()\n",
    "\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"')\n",
    "            next(reader, None)  # Skip the header\n",
    "            for idx, line in enumerate(reader):\n",
    "                # Assuming the first column is station_id, and the second column is the review text.\n",
    "                text = line[2].lower()\n",
    "                # Read the rest of the columns as labels (multi-label for each row)\n",
    "                label = [float(label) for label in line[3:]]  # Adjusted to read multiple labels\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = np.array(labels)  # Convert labels to a numpy array for easier handling\n",
    "\n",
    "        self.dict = corpora.Dictionary.load(dict_path)\n",
    "        # self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "        #                         usecols=[0]).values\n",
    "        # self.dict = list(self.dict.token2id.keys())\n",
    "        # self.dict = [word[0] for word in self.dict]\n",
    "        self.max_length_sentences = max_length_sentences\n",
    "        self.max_length_word = max_length_word\n",
    "        self.num_classes = self.labels.shape[1]  # Adjusted to get the number of classes from the label shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]  # Now label is a vector\n",
    "        text = self.texts[index]\n",
    "        document_encode = [\n",
    "            [self.dict.token2id[word] if word in self.dict.token2id else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            in sent_tokenize(text=text)]\n",
    "\n",
    "            # [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            # in sent_tokenize(text=text)]\n",
    "\n",
    "        for sentences in document_encode:\n",
    "            if len(sentences) < self.max_length_word:\n",
    "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
    "                sentences.extend(extended_words)\n",
    "\n",
    "        if len(document_encode) < self.max_length_sentences:\n",
    "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
    "                                  range(self.max_length_sentences - len(document_encode))]\n",
    "            document_encode.extend(extended_sentences)\n",
    "\n",
    "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
    "                          :self.max_length_sentences]\n",
    "\n",
    "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
    "        document_encode += 1\n",
    "\n",
    "        return document_encode.astype(np.int64), label.astype(np.float32)  # Ensure correct data type for labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttNet(nn.Module):\n",
    "    def __init__(self, word2vec_path, hidden_size=50):\n",
    "        super(WordAttNet, self).__init__()\n",
    "\n",
    "        # dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
    "        # dict_len, embed_size = dict.shape\n",
    "        # dict_len += 1\n",
    "        \n",
    "        # unknown_word = np.zeros((1, embed_size))\n",
    "        # dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n",
    "\n",
    "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
    "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "\n",
    "        self.lookup = torch.load(word2vec_path)\n",
    "        # self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
    "        \n",
    "        embed_size = self.lookup.embedding_dim\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "\n",
    "        self.word_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        output = self.lookup(input)\n",
    "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
    "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output,output.permute(1,0))\n",
    "\n",
    "        return output, h_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from utils import matrix_mul, element_wise_mul\n",
    "\n",
    "class SentAttNet(nn.Module):\n",
    "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n",
    "        super(SentAttNet, self).__init__()\n",
    "\n",
    "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
    "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
    "\n",
    "        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
    "        # self.sent_softmax = nn.Softmax()\n",
    "        # self.fc_softmax = nn.Softmax()\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        self.sent_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        f_output, h_output = self.gru(input, hidden_state)\n",
    "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, h_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAttNet(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n",
    "                 max_sent_length, max_word_length):\n",
    "        super(HierAttNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.max_sent_length = max_sent_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n",
    "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
    "        self._init_hidden_state()\n",
    "\n",
    "    def _init_hidden_state(self, last_batch_size=None):\n",
    "        if last_batch_size:\n",
    "            batch_size = last_batch_size\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
    "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
    "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output_list = []\n",
    "        input = input.permute(1, 0, 2)\n",
    "        for i in input:\n",
    "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n",
    "            output_list.append(output)\n",
    "        output = torch.cat(output_list, 0)\n",
    "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import get_max_lengths, get_evaluation\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_set = '../data/train.csv'\n",
    "test_set = '../data/test.csv'\n",
    "word2vec_path = '../data/embedding_layer.pth'\n",
    "dict_path = '../data/comments.dict'\n",
    "word_hidden_size = 50\n",
    "sent_hidden_size = 50\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "num_epoches = 2\n",
    "test_interval = 50\n",
    "es_min_delta = 0.0\n",
    "es_patience = 5\n",
    "\n",
    "training_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": True,\n",
    "                   \"drop_last\": True}\n",
    "\n",
    "test_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "\n",
    "\n",
    "max_word_length, max_sent_length = get_max_lengths(train_set)\n",
    "training_set = MyDataset(train_set, dict_path, max_sent_length, max_word_length)\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "test_set = MyDataset(test_set, dict_path, max_sent_length, max_word_length)\n",
    "test_generator = DataLoader(test_set, **test_params)\n",
    "model = HierAttNet(word_hidden_size, sent_hidden_size, batch_size, training_set.num_classes,\n",
    "                   word2vec_path, max_sent_length, max_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=0.01)\n",
    "best_loss = 1e5\n",
    "best_epoch = 0\n",
    "model.train()\n",
    "num_iter_per_epoch = len(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for iter, (feature, label) in enumerate(training_generator):\n",
    "        if torch.cuda.is_available():\n",
    "            feature = feature.cuda()\n",
    "            label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model._init_hidden_state()\n",
    "        predictions = model(feature)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_metrics = get_modified_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"hamming_loss\"], threshold=0.25)\n",
    "        print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Hamming Loss: {}\".format(\n",
    "            epoch + 1,\n",
    "            num_epoches,\n",
    "            iter + 1,\n",
    "            num_iter_per_epoch,\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            loss, training_metrics[\"hamming_loss\"]))\n",
    "        # writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
    "    if epoch % test_interval == 0:\n",
    "        model.eval()\n",
    "        loss_ls = []\n",
    "        te_label_ls = []\n",
    "        te_pred_ls = []\n",
    "        for te_feature, te_label in test_generator:\n",
    "            num_sample = len(te_label)\n",
    "            if torch.cuda.is_available():\n",
    "                te_feature = te_feature.cuda()\n",
    "                te_label = te_label.cuda()\n",
    "            with torch.no_grad():\n",
    "                model._init_hidden_state(num_sample)\n",
    "                te_predictions = model(te_feature)\n",
    "            te_loss = criterion(te_predictions, te_label)\n",
    "            loss_ls.append(te_loss * num_sample)\n",
    "            te_label_ls.extend(te_label.clone().cpu())\n",
    "            te_pred_ls.append(te_predictions.clone().cpu())\n",
    "        te_loss = sum(loss_ls) / test_set.__len__()\n",
    "        te_pred = torch.cat(te_pred_ls, 0)\n",
    "        te_label = np.array([te_label_ten.numpy() for te_label_ten in te_label_ls])\n",
    "        test_metrics = get_modified_evaluation(te_label, te_pred.numpy(), list_metrics=[\"hamming_loss\"], threshold=0.25) \n",
    "        # test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"loss\"]) #list_metrics=[\"accuracy\", \"confusion_matrix\"]\n",
    "        # output_file.write(\n",
    "        #     \"Epoch: {}/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n\".format(\n",
    "        #         epoch + 1, num_epoches,\n",
    "        #         te_loss,\n",
    "        #         test_metrics[\"accuracy\"],\n",
    "        #         test_metrics[\"confusion_matrix\"]))\n",
    "        print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Hamming Loss: {}\".format(\n",
    "            epoch + 1,\n",
    "            num_epoches,\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            te_loss, test_metrics[\"hamming_loss\"]))\n",
    "        # writer.add_scalar('Test/Loss', te_loss, epoch)\n",
    "        # writer.add_scalar('Test/Accuracy', test_metrics[\"accuracy\"], epoch)\n",
    "        model.train()\n",
    "        # if te_loss + es_min_delta < best_loss:\n",
    "        #     best_loss = te_loss\n",
    "        #     best_epoch = epoch\n",
    "        #     torch.save(model, saved_path + os.sep + \"whole_model_han\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch > es_patience > 0:\n",
    "            print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
